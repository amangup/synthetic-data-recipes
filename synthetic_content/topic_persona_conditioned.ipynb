{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbcf6e57-1c59-44c3-8710-a3fa927b4046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from jinja2 import Template\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0107fb4c-5a46-4dbb-8774-209b71168555",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 1024\n",
    "NUM_GPUS = 4\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2fdeca6-d6b4-4d2f-a438-6394da58945d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef0be8a32734ed0a3d5287227990e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset('wikimedia/wikipedia', name='20231101.en', split='train', streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "219f2b00-3fdd-4541-908e-69439524ed39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Komorica\n",
      "Glassport Odds\n",
      "Ciudad Nueva (Hato Rey)\n",
      "Kamiokite\n",
      "Roobaka\n",
      "Wayne Ormond\n",
      "The Pagans (film)\n",
      "Alfred A. Gilman\n",
      "1922 Austin twin tornadoes\n",
      "Gornji Emovci\n"
     ]
    }
   ],
   "source": [
    "for row in ds.shuffle(seed=1998, buffer_size=10_000).take(10):\n",
    "    print(row['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a9d9a50-7876-49d5-8f18-27558e4584c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"topic_persona_conditioned.jinja2\") as f:\n",
    "    template_str = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3379682b-e945-48ce-bfea-b689076cb0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"topic_persona_content_shots.json\") as f:\n",
    "    content_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72c1d4ac-3355-43e0-b349-f4ab970c48ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in content_json:\n",
    "    c['persona'] = json.dumps(c['persona'], indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79c11090-d732-4015-9040-644c7ff0f0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = Template(template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8196360-e59b-4ce2-bbcd-c012f0ae1405",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = template.render(contents=content_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d3cb5f4-7420-473f-9cfc-a0aa249c12f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Instructions\n",
      "\n",
      "Your goal is to write content about a topic.\n",
      "The input will contain a persona, and content should appear that it is spoken by that person.\n",
      "\n",
      "# Output Instructions\n",
      "\n",
      "Respond with the content in plain text, with no structure.\n",
      "\n",
      "# Examples\n",
      "\n",
      "Persona:\n",
      "{\n",
      "  \"identity\": \"Gordon James Ramsay, 57-year-old celebrity chef turned global hospitality mogul, embodies the dual nature of culinary artistry and unrelenting business acumen. His explosive temper and exacting standards in professional kitchens contrast sharply with his nurturing approach to amateur cooks and children. A former professional footballer whose career was cut short by injury, he channels the intensity of athletic competition into culinary excellence, driving himself and others toward perpetual improvement with both militaristic discipline and surprising moments of profound empathy.\",\n",
      "  \"personalLife\": \"Living between London and Los Angeles with his wife Tana and five children, Gordon maintains a strict separation between his volcanic professional persona and his role as a protective, occasionally overindulgent father. He takes immense pride in his self-made success yet remains haunted by his childhood experiences with an alcoholic, abusive father and financial instability. Despite his wealth, he still counts the pennies and insists his children fly economy class, driven by both principle and lingering anxiety about financial security.\",\n",
      "  \"communication\": \"His language oscillates between scorching professional criticism ('You fucking donkey!') and gentle encouragement to aspiring chefs ('Have faith in your abilities'). His extensive cursing serves as both stress release and tactical tool, deliberately wielded for impact rather than mere habit. In private settings, particularly with family or struggling restaurateurs, he displays a quieter, more reflective communication style that few expect. His Scottish accent thickens noticeably when emotional, whether in rage or tenderness.\",\n",
      "  \"background\": \"Raised in a troubled home in Stratford-upon-Avon, he found early structure in football before an injury redirected him to kitchens. His culinary journey from commis to Michelin-starred chef was marked by intense apprenticeships under notoriously demanding mentors like Marco Pierre White and Albert Roux. These experiences shaped his belief that greatness requires both suffering and precision, though he's evolved to recognize different paths to excellence. The trauma of his early family life manifests in his fierce protection of both his own family and the families of struggling restaurateurs.\",\n",
      "  \"dailyLife\": \"Maintains a punishing physical regime, competing in marathons and Ironman events, viewing physical discipline as inseparable from culinary excellence. Starts each day with a cold shower and protein breakfast, treating his body like a professional athlete's while running a global empire. Despite his fame, still drops into his restaurants unannounced, tasting sauces and inspecting stations with the same intensity he had as a young chef. Religiously calls his children daily, no matter which time zone he's in.\",\n",
      "  \"coping\": \"Manages the intense pressure of his empire through extreme exercise and carefully structured family time. His infamous temper, while genuine, is also a pressure release valve that prevents deeper frustrations from affecting his home life. Deals with stress through high-intensity workouts and quiet early morning recipe testing, finding peace in the precise execution of classic dishes. His perfectionistic tendencies, while valuable professionally, require constant management to prevent them from overwhelming his personal relationships.\",\n",
      "  \"interests\": \"Beyond cooking, he's obsessed with physical fitness and sports, particularly football, using these as both stress relief and connection points with his children. Has a surprising passion for interior design and plate presentation, seeing visual aesthetics as crucial to the dining experience. Privately studies business management and leadership, constantly working to evolve his management style beyond the aggressive approach that made him famous. Maintains a deep interest in food science and innovation, though he publicly emphasizes traditional techniques.\",\n",
      "  \"relationships\": \"Maintains a complex web of professional relationships characterized by intense loyalty to those who meet his standards and explosive confrontations with those who don't. His marriage to Tana represents a stable partnership that has weathered public scrutiny and personal tragedies, including a late-term miscarriage they've spoken about openly. With his children, he's deliberately different from his own father, present and supportive while still maintaining high expectations. His professional kitchen relationships often evolve from feared taskmaster to trusted mentor, reflecting his belief in tough love as a path to excellence.\",\n",
      "  \"values\": \"Holds an unshakeable belief in the transformative power of excellent food and proper technique. Views cooking as a meritocracy where background matters less than dedication and skill. Emphasizes honesty, even when brutal, as fundamental to both cooking and life. Despite his wealth, values authenticity and hard work over luxury, often preferring simple, perfectly executed dishes to elaborate creations. Believes passionately in the importance of family meals and cooking as a way to bring people together.\",\n",
      "  \"aspirations\": \"Beyond maintaining his culinary empire, seeks to leave a legacy of transformed lives through both his charitable foundation and his influence on the restaurant industry. Privately works toward creating a more sustainable model of chef training that maintains excellence while reducing the psychological toll of traditional kitchen hierarchies. Dreams of his children finding their own passions rather than feeling pressured to follow his path, though he secretly hopes some will choose the culinary world.\",\n",
      "  \"dialogue\": \"In professional kitchens: 'It's RAW, you muppet! You'll kill someone!' To struggling restaurant owners: 'If you don't get a grip on your business, your whole family loses everything - I've been there.' Teaching children: 'Perfect? No. But you're learning, and that's brilliant.' To his own children: 'I'm not paying for first class because you need to understand value, not privilege.' During quiet moments: 'Perfection doesn't exist, but the pursuit of it does, and that's what drives us all forward.'\"\n",
      "}\n",
      "\n",
      "Topic:\n",
      "Gordon Ramsay's early career and motivations\n",
      "\n",
      "Content:\n",
      "Fucking hell. That's a big question. Legacy? I think my legacy would be the plethora of talent that I've had the pleasure to work with, they’re now laying the foundation of the future. Oh, and whether I’m in a tapas bar on the backstreets of Barcelona or the arse-end of the jungle, someone wants to talk to me about my fucking beef Wellington. As for what I'd like to be remembered for... I'd like to be remembered for perfection. Because it's an absolute bitch to get right, but when it is, it's the most beautiful fucking thing on the planet.\n",
      "The beginning was the upset at Rangers [FC]. I’d been selected for a testimonial game, a childhood dream. I remember everything; seeing the team sheet, even the fucking pin on the board. Number 3, Ramsay. I was fucking starting! 17 minutes in, I got tackled and I was fucked. I knew there and then. Fucked.\n",
      "My mum taught me: “Don’t make excuses. Excuses get you nowhere.” The woman was working three jobs, Dad was an alcoholic, so who was I to argue? I dusted myself down. I needed an out, so London it was. I started working as a second commis at The Mayfair Hotel. One day, I had to cover the night shift after working a double. Fucking club sandwich, fucking fries, fucking grilled cheese. Basic shit you see in every five-star hotel. It got to 6.30am, I’d been working 24 hours straight; I went to the staff canteen with a coffee and a copy of The Caterer. And the fucking guy on the cover was Marco Pierre White.\n",
      "I went straight to the telephone box and phoned him. He answered, asked where I was working, and I told him. “How many Michelin stars does it have?” he asked. None, of course. He gave me a lot of shit, but eventually said, “OK, get on a bus to Wandsworth.” So I did. Two hours later, I’m being interviewed by him. He was composing this ravioli dish and it’s still one of the most intimidating things I've ever seen in my life. But it was so clear that I needed to be learning from him.\n",
      "Marcus Wareing described me as a game changer...a cooking machine of pure focus and adrenaline. That's very endearing. See, all the bullshit in the press… Marcus fell out with my father-in-law, he didn't fall out with me. I love the guy dearly, he's a super-talented chef and I was Best Man at his wedding, for God's sake. We went to hell and back in the early days.\n",
      "I had this raw instinct. Every time I accepted a job – from Marco to Gavroche to the Roux brothers – I wanted to drop down a level to learn more. Everyone thinks promotion is the only natural progression, but it’s the opposite. Forget the titles; to learn, you need to get lower.\n",
      "You target those titles at different stages in your life. We’re celebrating 25 years of Restaurant Gordon Ramsay this year. In 1998, I sold our flat to raise the £1 million to buy the place from Pierre Koffman. We put it all on the line; if it didn’t work, we’d be homeless. There was no plan B.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fc20c3e-14e0-458e-b506-8131e0733f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b811287aeb74e8e9c772d3872ab72d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/862 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc9dddf7e744a43b8285a9d6dba0cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/18.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f0e0aa4c474093b5e60b405d28a84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "personas_ds = load_dataset('amang1802/personas_sample_405B')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "838f279e-b2e4-47ef-ae87-ad32b8f89295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d31425c2b4404da32022a87f26f7d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "personas_deduped = personas_ds.filter(lambda row: row['is_cluster_centroid'] == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "604c3ff2-8942-41c1-8e99-55502c60547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "personas_sampled = personas_ds.shuffle(seed=1998).select(range(personas_deduped.num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b476cdfe-9dfd-4ca3-9779-d195bf56717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_ds = Dataset.from_list(list(ds.shuffle(seed=1998, buffer_size=1000_000).take(NUM_TOPICS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b290be9f-a21f-4d71-80c9-908470b53d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def persona_cross_product(int_ids, titles, personas):\n",
    "    num_personas = personas.num_rows\n",
    "    \n",
    "    return {\n",
    "        \"id\": [int_ids[0]] * num_personas,\n",
    "        \"title\": [titles[0]] * num_personas,\n",
    "        \"persona_id\": personas['id'],\n",
    "        \"persona\": personas['persona']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aeaf367a-0863-428a-9449-1c39ef13307b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c89a20b6ea3450eaf1506c355eee9d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1024 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wiki_personas_sampled_ds = wiki_ds.map(lambda i, t: persona_cross_product(i, t, personas_sampled),\n",
    "                                       input_columns=['id', 'title'], remove_columns=ds.column_names, batched=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4059d854-516b-475d-9c85-f7331ef97003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b565837a96744601b7d16211a6793b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1024 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wiki_personas_deduped_ds = wiki_ds.map(lambda i, t: persona_cross_product(i, t, personas_deduped),\n",
    "                                       input_columns=['id', 'title'], remove_columns=ds.column_names, batched=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74bca127-e8e3-4443-81ea-2f812f848d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'title', 'persona_id', 'persona'],\n",
       "     num_rows: 25600\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'title', 'persona_id', 'persona'],\n",
       "     num_rows: 25600\n",
       " }))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_personas_sampled_ds, wiki_personas_deduped_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5078a1c4-c45f-4ff8-9c80-a0e6b96a5570",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.1-405B-Instruct-FP8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fcb5c2a-cc3d-4c75-be84-df1959e4bd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8a56b4b-5c8c-4f2f-ac1f-9c68337d7154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-24 08:49:59 config.py:478] This model supports multiple tasks: {'score', 'generate', 'reward', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 12-24 08:50:00 config.py:1216] Defaulting to use mp for distributed inference\n",
      "INFO 12-24 08:50:00 config.py:1364] Chunked prefill is enabled with max_num_batched_tokens=4096.\n",
      "INFO 12-24 08:50:00 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='meta-llama/Llama-3.1-405B-Instruct-FP8', speculative_config=None, tokenizer='meta-llama/Llama-3.1-405B-Instruct-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fbgemm_fp8, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-405B-Instruct-FP8, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[32,24,16,8,4,2,1],\"max_capture_size\":32}, use_cached_outputs=False, \n",
      "WARNING 12-24 08:50:00 multiproc_worker_utils.py:312] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 12-24 08:50:00 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113434)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=113434)\u001b[0;0m INFO 12-24 08:50:00 selector.py:120] Using Flash Attention backend.\n",
      "INFO 12-24 08:50:00 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113435)\u001b[0;0m INFO 12-24 08:50:00 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113435)\u001b[0;0m INFO 12-24 08:50:00 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "INFO 12-24 08:50:00 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113438)\u001b[0;0m INFO 12-24 08:50:00 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113438)\u001b[0;0m INFO 12-24 08:50:00 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "INFO 12-24 08:50:03 utils.py:922] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113435)\u001b[0;0m INFO 12-24 08:50:03 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113434)\u001b[0;0m INFO 12-24 08:50:03 utils.py:922] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113438)\u001b[0;0m INFO 12-24 08:50:03 utils.py:922] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113435)\u001b[0;0m INFO 12-24 08:50:03 utils.py:922] Found nccl from library libnccl.so.2\n",
      "INFO 12-24 08:50:03 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113434)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=113438)\u001b[0;0m INFO 12-24 08:50:03 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-24 08:50:03 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-24 08:50:05 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113435)\u001b[0;0m INFO 12-24 08:50:05 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113434)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=113438)\u001b[0;0m INFO 12-24 08:50:05 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 12-24 08:50:05 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 12-24 08:50:05 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_817087c4'), local_subscribe_port=39485, remote_subscribe_port=None)\n",
      "INFO 12-24 08:50:05 model_runner.py:1092] Starting to load model meta-llama/Llama-3.1-405B-Instruct-FP8...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113434)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=113435)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=113438)\u001b[0;0m INFO 12-24 08:50:05 model_runner.py:1092] Starting to load model meta-llama/Llama-3.1-405B-Instruct-FP8...\n",
      "INFO 12-24 08:50:05 model_runner.py:1092] Starting to load model meta-llama/Llama-3.1-405B-Instruct-FP8...\n",
      "INFO 12-24 08:50:05 model_runner.py:1092] Starting to load model meta-llama/Llama-3.1-405B-Instruct-FP8...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113435)\u001b[0;0m INFO 12-24 08:50:06 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113434)\u001b[0;0m INFO 12-24 08:50:06 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 12-24 08:50:06 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113438)\u001b[0;0m INFO 12-24 08:50:06 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b1d29c8a964032b8fa24ac0b903bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/109 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=113435)\u001b[0;0m INFO 12-24 08:52:24 model_runner.py:1097] Loading model weights took 113.4847 GB\n",
      "INFO 12-24 08:52:25 model_runner.py:1097] Loading model weights took 113.4847 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113438)\u001b[0;0m INFO 12-24 08:52:25 model_runner.py:1097] Loading model weights took 113.4847 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113434)\u001b[0;0m INFO 12-24 08:52:25 model_runner.py:1097] Loading model weights took 113.4847 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113435)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=113434)\u001b[0;0m INFO 12-24 08:52:29 worker.py:241] Memory profiling takes 4.09 seconds\n",
      "INFO 12-24 08:52:29 worker.py:241] Memory profiling takes 4.09 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113435)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=113434)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=113438)\u001b[0;0m INFO 12-24 08:52:29 worker.py:241] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.98) = 136.92GiB\n",
      "INFO 12-24 08:52:29 worker.py:241] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.98) = 136.92GiB\n",
      "INFO 12-24 08:52:29 worker.py:241] Memory profiling takes 4.09 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113435)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=113434)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=113438)\u001b[0;0m INFO 12-24 08:52:29 worker.py:241] model weights take 113.48GiB; non_torch_memory takes 4.15GiB; PyTorch activation peak memory takes 0.76GiB; the rest of the memory reserved for KV Cache is 18.53GiB.\n",
      "INFO 12-24 08:52:29 worker.py:241] model weights take 113.48GiB; non_torch_memory takes 4.15GiB; PyTorch activation peak memory takes 0.76GiB; the rest of the memory reserved for KV Cache is 18.53GiB.\n",
      "INFO 12-24 08:52:29 worker.py:241] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.98) = 136.92GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113438)\u001b[0;0m INFO 12-24 08:52:29 worker.py:241] model weights take 113.48GiB; non_torch_memory takes 3.68GiB; PyTorch activation peak memory takes 0.76GiB; the rest of the memory reserved for KV Cache is 19.00GiB.\n",
      "INFO 12-24 08:52:29 worker.py:241] Memory profiling takes 4.23 seconds\n",
      "INFO 12-24 08:52:29 worker.py:241] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.98) = 136.92GiB\n",
      "INFO 12-24 08:52:29 worker.py:241] model weights take 113.48GiB; non_torch_memory takes 4.93GiB; PyTorch activation peak memory takes 0.76GiB; the rest of the memory reserved for KV Cache is 17.75GiB.\n",
      "INFO 12-24 08:52:29 distributed_gpu_executor.py:57] # GPU blocks: 9231, # CPU blocks: 2080\n",
      "INFO 12-24 08:52:29 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 36.06x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113434)\u001b[0;0m INFO 12-24 08:52:33 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113434)\u001b[0;0m INFO 12-24 08:52:33 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-24 08:52:33 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-24 08:52:33 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113435)\u001b[0;0m INFO 12-24 08:52:34 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113435)\u001b[0;0m INFO 12-24 08:52:34 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113438)\u001b[0;0m INFO 12-24 08:52:34 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113438)\u001b[0;0m INFO 12-24 08:52:34 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113438)\u001b[0;0m INFO 12-24 08:52:38 custom_all_reduce.py:224] Registering 1771 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113435)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=113434)\u001b[0;0m INFO 12-24 08:52:38 custom_all_reduce.py:224] Registering 1771 cuda graph addresses\n",
      "INFO 12-24 08:52:38 custom_all_reduce.py:224] Registering 1771 cuda graph addresses\n",
      "INFO 12-24 08:52:38 custom_all_reduce.py:224] Registering 1771 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113435)\u001b[0;0m INFO 12-24 08:52:38 model_runner.py:1527] Graph capturing finished in 4 secs, took 0.20 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113434)\u001b[0;0m INFO 12-24 08:52:38 model_runner.py:1527] Graph capturing finished in 4 secs, took 0.20 GiB\n",
      "INFO 12-24 08:52:38 model_runner.py:1527] Graph capturing finished in 4 secs, took 0.20 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=113438)\u001b[0;0m INFO 12-24 08:52:38 model_runner.py:1527] Graph capturing finished in 4 secs, took 0.20 GiB\n",
      "INFO 12-24 08:52:38 llm_engine.py:446] init engine (profile, create kv cache, warmup model) took 13.03 seconds\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=model_id, max_model_len=4096, tensor_parallel_size=NUM_GPUS, gpu_memory_utilization=0.98, enable_chunked_prefill=True, max_num_batched_tokens=4096, max_num_seqs=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be81ccc6-6107-4b54-bf44-5749ea2d684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_content(topics, personas):\n",
    "    personas_str = [json.dumps(persona, indent=2) for persona in personas]\n",
    "    messages = [[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": \"Persona\\n\" + persona + \"\\n\\nTopic:\\n\" + topic + \"\\n\\nContent:\"}]\n",
    "                for topic, persona in zip(topics, personas_str)]\n",
    "    prompts = [tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True) for chat in messages]\n",
    "\n",
    "    outputs = llm.generate(prompts, SamplingParams(temperature=0.25, top_p=0.9, max_tokens=3072))\n",
    "\n",
    "    return {\"synthetic_content\": [output.outputs[0].text.strip() for output in outputs]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f11b25-3337-43bd-9441-83034a216d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function generate_content at 0x7deec81c9360> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c170c2752841fa9adfe9275df0b461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Acessed prompts:   0% 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "\u001b[Acessed prompts:   0% 1/1024 [00:31<8:51:00, 31.14s/it, est. speed input: 95.33 toks/s, output: 3.66 toks/s]\n",
      "\u001b[Acessed prompts:   0% 2/1024 [00:33<4:06:16, 14.46s/it, est. speed input: 175.01 toks/s, output: 8.22 toks/s]\n",
      "\u001b[Acessed prompts:   0% 3/1024 [00:35<2:24:12,  8.47s/it, est. speed input: 253.15 toks/s, output: 12.98 toks/s]\n",
      "\u001b[Acessed prompts:   0% 4/1024 [00:38<1:51:55,  6.58s/it, est. speed input: 306.24 toks/s, output: 17.61 toks/s]\n",
      "\u001b[Acessed prompts:   0% 5/1024 [00:40<1:19:04,  4.66s/it, est. speed input: 370.01 toks/s, output: 22.84 toks/s]\n",
      "\u001b[Acessed prompts:   1% 6/1024 [00:42<1:04:21,  3.79s/it, est. speed input: 417.46 toks/s, output: 27.96 toks/s]\n",
      "\u001b[Acessed prompts:   1% 7/1024 [00:43<48:09,  2.84s/it, est. speed input: 480.35 toks/s, output: 33.75 toks/s]  \n",
      "\u001b[Acessed prompts:   1% 8/1024 [00:44<37:14,  2.20s/it, est. speed input: 535.43 toks/s, output: 39.18 toks/s]\n",
      "\u001b[Acessed prompts:   1% 9/1024 [00:45<32:57,  1.95s/it, est. speed input: 585.36 toks/s, output: 44.19 toks/s]\n",
      "\u001b[Acessed prompts:   1% 10/1024 [00:46<29:56,  1.77s/it, est. speed input: 630.94 toks/s, output: 49.21 toks/s]\n",
      "\u001b[Acessed prompts:   1% 11/1024 [00:48<27:54,  1.65s/it, est. speed input: 675.22 toks/s, output: 54.09 toks/s]\n",
      "\u001b[Acessed prompts:   1% 12/1024 [00:49<23:39,  1.40s/it, est. speed input: 724.55 toks/s, output: 59.27 toks/s]\n",
      "\u001b[Acessed prompts:   1% 13/1024 [00:49<21:14,  1.26s/it, est. speed input: 769.37 toks/s, output: 64.29 toks/s]\n",
      "\u001b[Acessed prompts:   1% 14/1024 [00:50<19:01,  1.13s/it, est. speed input: 815.21 toks/s, output: 69.51 toks/s]\n",
      "\u001b[Acessed prompts:   1% 15/1024 [00:53<28:33,  1.70s/it, est. speed input: 825.33 toks/s, output: 71.85 toks/s]\n",
      "\u001b[Acessed prompts:   2% 16/1024 [00:55<28:42,  1.71s/it, est. speed input: 851.61 toks/s, output: 76.26 toks/s]\n",
      "\u001b[Acessed prompts:   2% 17/1024 [00:56<25:42,  1.53s/it, est. speed input: 885.63 toks/s, output: 81.13 toks/s]\n",
      "\u001b[Acessed prompts:   2% 18/1024 [00:57<22:23,  1.34s/it, est. speed input: 923.07 toks/s, output: 86.15 toks/s]\n",
      "\u001b[Acessed prompts:   2% 19/1024 [00:58<22:19,  1.33s/it, est. speed input: 949.53 toks/s, output: 90.70 toks/s]\n",
      "\u001b[Acessed prompts:   2% 20/1024 [00:59<20:16,  1.21s/it, est. speed input: 986.96 toks/s, output: 95.80 toks/s]\n",
      "\u001b[Acessed prompts:   2% 21/1024 [01:00<19:01,  1.14s/it, est. speed input: 1020.57 toks/s, output: 100.51 toks/s]\n",
      "\u001b[Acessed prompts:   2% 22/1024 [01:01<17:57,  1.08s/it, est. speed input: 1051.66 toks/s, output: 105.30 toks/s]\n",
      "\u001b[Acessed prompts:   2% 23/1024 [01:03<20:11,  1.21s/it, est. speed input: 1071.66 toks/s, output: 109.33 toks/s]\n",
      "\u001b[Acessed prompts:   2% 24/1024 [01:04<20:02,  1.20s/it, est. speed input: 1100.07 toks/s, output: 113.63 toks/s]\n",
      "\u001b[Acessed prompts:   2% 25/1024 [01:06<22:39,  1.36s/it, est. speed input: 1117.88 toks/s, output: 117.12 toks/s]\n",
      "\u001b[Acessed prompts:   3% 26/1024 [01:07<21:29,  1.29s/it, est. speed input: 1143.39 toks/s, output: 121.54 toks/s]\n",
      "\u001b[Acessed prompts:   3% 27/1024 [01:08<21:07,  1.27s/it, est. speed input: 1165.60 toks/s, output: 126.01 toks/s]\n",
      "\u001b[Acessed prompts:   3% 28/1024 [01:10<22:45,  1.37s/it, est. speed input: 1182.10 toks/s, output: 129.74 toks/s]\n",
      "\u001b[Acessed prompts:   3% 29/1024 [01:10<20:02,  1.21s/it, est. speed input: 1209.11 toks/s, output: 134.74 toks/s]\n",
      "\u001b[Acessed prompts:   3% 30/1024 [01:13<26:38,  1.61s/it, est. speed input: 1203.14 toks/s, output: 137.00 toks/s]\n",
      "\u001b[Acessed prompts:   3% 31/1024 [01:14<23:55,  1.45s/it, est. speed input: 1221.35 toks/s, output: 139.71 toks/s]\n",
      "\u001b[Acessed prompts:   3% 32/1024 [01:15<21:19,  1.29s/it, est. speed input: 1245.90 toks/s, output: 144.87 toks/s]\n",
      "\u001b[Acessed prompts:   3% 33/1024 [01:16<19:31,  1.18s/it, est. speed input: 1269.70 toks/s, output: 149.74 toks/s]\n",
      "\u001b[Acessed prompts:   3% 34/1024 [01:17<20:29,  1.24s/it, est. speed input: 1285.08 toks/s, output: 152.30 toks/s]\n",
      "\u001b[Acessed prompts:   3% 35/1024 [01:18<19:55,  1.21s/it, est. speed input: 1306.32 toks/s, output: 153.83 toks/s]\n",
      "\u001b[Acessed prompts:   4% 36/1024 [01:20<22:07,  1.34s/it, est. speed input: 1317.70 toks/s, output: 152.69 toks/s]\n",
      "\u001b[Acessed prompts:   4% 37/1024 [01:22<25:16,  1.54s/it, est. speed input: 1321.14 toks/s, output: 153.90 toks/s]"
     ]
    }
   ],
   "source": [
    "syn_sampled_ds = wiki_personas_sampled_ds.map(generate_content, batched=True, batch_size=NUM_TOPICS, input_columns=[\"title\", \"persona\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05908342-c250-409d-ada1-14def27199f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_sampled_ds.push_to_hub('amang1802/wiki_topic_persona_sampled_405B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0015184-66ed-4e58-9a8d-360f256acf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_deduped_ds = wiki_personas_deduped_ds.map(generate_content, batched=True, batch_size=NUM_TOPICS, input_columns=[\"title\", \"persona\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33743e06-e525-44c7-a2f4-6d70366b90ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_deduped_ds.push_to_hub('amang1802/wiki_topic_persona_deduped_405B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daeea6a-a626-46c7-b0fc-5f478aecabef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
