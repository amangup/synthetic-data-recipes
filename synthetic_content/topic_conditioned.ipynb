{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbcf6e57-1c59-44c3-8710-a3fa927b4046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from jinja2 import Template\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0107fb4c-5a46-4dbb-8774-209b71168555",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 1024\n",
    "NUM_GPUS = 4\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2fdeca6-d6b4-4d2f-a438-6394da58945d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042a81d6371a4c89afb41570d5c236c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset('wikimedia/wikipedia', name='20231101.en', split='train', streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "219f2b00-3fdd-4541-908e-69439524ed39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Komorica\n",
      "Glassport Odds\n",
      "Ciudad Nueva (Hato Rey)\n",
      "Kamiokite\n",
      "Roobaka\n",
      "Wayne Ormond\n",
      "The Pagans (film)\n",
      "Alfred A. Gilman\n",
      "1922 Austin twin tornadoes\n",
      "Gornji Emovci\n"
     ]
    }
   ],
   "source": [
    "for row in ds.shuffle(seed=1998, buffer_size=10_000).take(10):\n",
    "    print(row['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a9d9a50-7876-49d5-8f18-27558e4584c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"topic_conditioned.jinja2\") as f:\n",
    "    template_str = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3379682b-e945-48ce-bfea-b689076cb0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"topic_content_shots.json\") as f:\n",
    "    content_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79c11090-d732-4015-9040-644c7ff0f0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = Template(template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8196360-e59b-4ce2-bbcd-c012f0ae1405",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = template.render(contents=content_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d3cb5f4-7420-473f-9cfc-a0aa249c12f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Instructions\n",
      "\n",
      "Imagine you're an expert on the topic given by the user. Your goal is to write an article explain the topic in detail.\n",
      "\n",
      "# Output Instructions\n",
      "\n",
      "Respond with the content in plain text, with no structure.\n",
      "\n",
      "# Examples\n",
      "\n",
      "Topic:\n",
      "Gordon Ramsay's early career\n",
      "\n",
      "Content:\n",
      "Gordon James Ramsay was born in Johnstone, Scotland, on 8 November 1966, the son of Helen (née Cosgrove), a nurse, and Gordon James Sr., who worked as a swimming pool manager, welder, and shopkeeper. He has an older sister, a younger brother, and a younger sister. When he was nine years old, he moved with his family to England and grew up in the Bishopton area of Stratford-upon-Avon. He has described his early life as 'hopelessly itinerant' and said his family moved constantly owing to the aspirations and failures of his father, who was an occasionally violent alcoholic; Ramsay described him as a 'hard-drinking womaniser'. In his autobiography, he revealed that his father abused and neglected the children and his mother. He worked as a pot washer in a local Indian restaurant where his sister was a waitress. He had hoped to become a footballer and was first chosen to play under-14 football at the age of 12, but his early footballing career was marked by injuries; after a serious knee injury, he was forced to give it up. At the age of 16, he moved out of the family home and into an apartment in Banbury.\n",
      "Ramsay's interest in cooking began in his teenage years; rather than be known as 'the football player with the gammy knee', he decided to pay more serious attention to his culinary education at age 19. Ramsay enrolled at North Oxfordshire Technical College, sponsored by the Rotarians, to study hotel management. He described his decision to enter catering college as 'a complete accident'.\n",
      "In the mid-1980s, Ramsay worked as a commis chef at the Wroxton House Hotel. He ran the kitchen and 60-seat dining room at the Wickham Arms until he quit after having sex with the owner's wife. Ramsay then moved to London, where he worked in a series of restaurants until being inspired to work for Marco Pierre White at Harveys.\n",
      "After working at Harveys for two years and ten months, Ramsay, tired of 'the rages and the bullying and violence', decided that the way to further advance his career was to study French cuisine. White discouraged Ramsay from taking a job in Paris, instead encouraging him to work for Albert Roux at Le Gavroche in Mayfair. Ramsay decided to take his advice, and there, Ramsay met Jean-Claude Breton, who later became his maître d'hôtel at Restaurant Gordon Ramsay. After Ramsay worked at Le Gavroche for a year, Roux invited him to work with him at Hotel Diva, a ski resort in the French Alps, as his number two. From there, a 23-year-old Ramsay moved to Paris to work with Guy Savoy and Joël Robuchon, both Michelin-starred chefs. He continued his training in France for three years, before giving in to the physical and mental stress of the kitchens and taking a year to work as a personal chef on the private yacht Idlewild, based in Bermuda. The role on the boat saw him travel to Sicily and Sardinia, Italy, and learn about Italian cuisine.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5078a1c4-c45f-4ff8-9c80-a0e6b96a5570",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.1-405B-Instruct-FP8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fcb5c2a-cc3d-4c75-be84-df1959e4bd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8a56b4b-5c8c-4f2f-ac1f-9c68337d7154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-24 05:58:37 config.py:478] This model supports multiple tasks: {'score', 'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 12-24 05:58:38 config.py:1216] Defaulting to use mp for distributed inference\n",
      "INFO 12-24 05:58:38 config.py:1364] Chunked prefill is enabled with max_num_batched_tokens=4096.\n",
      "INFO 12-24 05:58:38 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='meta-llama/Llama-3.1-405B-Instruct-FP8', speculative_config=None, tokenizer='meta-llama/Llama-3.1-405B-Instruct-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fbgemm_fp8, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-405B-Instruct-FP8, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[32,24,16,8,4,2,1],\"max_capture_size\":32}, use_cached_outputs=False, \n",
      "WARNING 12-24 05:58:38 multiproc_worker_utils.py:312] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 12-24 05:58:38 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 12-24 05:58:38 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92047)\u001b[0;0m INFO 12-24 05:58:38 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92047)\u001b[0;0m INFO 12-24 05:58:38 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92049)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=92048)\u001b[0;0m INFO 12-24 05:58:38 selector.py:120] Using Flash Attention backend.\n",
      "INFO 12-24 05:58:38 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92048)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=92049)\u001b[0;0m INFO 12-24 05:58:38 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "INFO 12-24 05:58:38 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "INFO 12-24 05:58:42 utils.py:922] Found nccl from library libnccl.so.2\n",
      "INFO 12-24 05:58:42 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92048)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=92047)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=92049)\u001b[0;0m INFO 12-24 05:58:42 utils.py:922] Found nccl from library libnccl.so.2\n",
      "INFO 12-24 05:58:42 utils.py:922] Found nccl from library libnccl.so.2\n",
      "INFO 12-24 05:58:42 utils.py:922] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92049)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=92048)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=92047)\u001b[0;0m INFO 12-24 05:58:42 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-24 05:58:42 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-24 05:58:42 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-24 05:58:44 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92048)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=92047)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=92049)\u001b[0;0m INFO 12-24 05:58:44 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 12-24 05:58:44 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 12-24 05:58:44 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 12-24 05:58:44 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_011cf3d9'), local_subscribe_port=38505, remote_subscribe_port=None)\n",
      "INFO 12-24 05:58:44 model_runner.py:1092] Starting to load model meta-llama/Llama-3.1-405B-Instruct-FP8...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92047)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=92049)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=92048)\u001b[0;0m INFO 12-24 05:58:44 model_runner.py:1092] Starting to load model meta-llama/Llama-3.1-405B-Instruct-FP8...\n",
      "INFO 12-24 05:58:44 model_runner.py:1092] Starting to load model meta-llama/Llama-3.1-405B-Instruct-FP8...\n",
      "INFO 12-24 05:58:44 model_runner.py:1092] Starting to load model meta-llama/Llama-3.1-405B-Instruct-FP8...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92048)\u001b[0;0m INFO 12-24 05:58:45 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 12-24 05:58:45 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92049)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=92047)\u001b[0;0m INFO 12-24 05:58:45 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 12-24 05:58:45 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2059798d204149ceaf078ed90e40d27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/109 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=92047)\u001b[0;0m INFO 12-24 06:00:58 model_runner.py:1097] Loading model weights took 113.4847 GB\n",
      "INFO 12-24 06:00:58 model_runner.py:1097] Loading model weights took 113.4847 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92048)\u001b[0;0m INFO 12-24 06:00:58 model_runner.py:1097] Loading model weights took 113.4847 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92049)\u001b[0;0m INFO 12-24 06:00:59 model_runner.py:1097] Loading model weights took 113.4847 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92048)\u001b[0;0m INFO 12-24 06:01:03 worker.py:241] Memory profiling takes 4.55 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92048)\u001b[0;0m INFO 12-24 06:01:03 worker.py:241] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.98) = 136.92GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92048)\u001b[0;0m INFO 12-24 06:01:03 worker.py:241] model weights take 113.48GiB; non_torch_memory takes 4.15GiB; PyTorch activation peak memory takes 0.76GiB; the rest of the memory reserved for KV Cache is 18.53GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92049)\u001b[0;0m INFO 12-24 06:01:03 worker.py:241] Memory profiling takes 4.55 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92049)\u001b[0;0m INFO 12-24 06:01:03 worker.py:241] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.98) = 136.92GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92049)\u001b[0;0m INFO 12-24 06:01:03 worker.py:241] model weights take 113.48GiB; non_torch_memory takes 3.68GiB; PyTorch activation peak memory takes 0.76GiB; the rest of the memory reserved for KV Cache is 19.00GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92047)\u001b[0;0m INFO 12-24 06:01:03 worker.py:241] Memory profiling takes 4.59 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92047)\u001b[0;0m INFO 12-24 06:01:03 worker.py:241] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.98) = 136.92GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92047)\u001b[0;0m INFO 12-24 06:01:03 worker.py:241] model weights take 113.48GiB; non_torch_memory takes 4.15GiB; PyTorch activation peak memory takes 0.76GiB; the rest of the memory reserved for KV Cache is 18.53GiB.\n",
      "INFO 12-24 06:01:03 worker.py:241] Memory profiling takes 4.65 seconds\n",
      "INFO 12-24 06:01:03 worker.py:241] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.98) = 136.92GiB\n",
      "INFO 12-24 06:01:03 worker.py:241] model weights take 113.48GiB; non_torch_memory takes 4.93GiB; PyTorch activation peak memory takes 0.76GiB; the rest of the memory reserved for KV Cache is 17.75GiB.\n",
      "INFO 12-24 06:01:03 distributed_gpu_executor.py:57] # GPU blocks: 9231, # CPU blocks: 2080\n",
      "INFO 12-24 06:01:03 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 36.06x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92048)\u001b[0;0m INFO 12-24 06:01:09 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92048)\u001b[0;0m INFO 12-24 06:01:09 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-24 06:01:09 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-24 06:01:09 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92047)\u001b[0;0m INFO 12-24 06:01:09 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92047)\u001b[0;0m INFO 12-24 06:01:09 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92049)\u001b[0;0m INFO 12-24 06:01:09 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92049)\u001b[0;0m INFO 12-24 06:01:09 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-24 06:01:13 custom_all_reduce.py:224] Registering 1771 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92049)\u001b[0;0m INFO 12-24 06:01:13 custom_all_reduce.py:224] Registering 1771 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92048)\u001b[0;0m INFO 12-24 06:01:13 custom_all_reduce.py:224] Registering 1771 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92047)\u001b[0;0m INFO 12-24 06:01:14 custom_all_reduce.py:224] Registering 1771 cuda graph addresses\n",
      "INFO 12-24 06:01:14 model_runner.py:1527] Graph capturing finished in 5 secs, took 0.20 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92047)\u001b[0;0m INFO 12-24 06:01:14 model_runner.py:1527] Graph capturing finished in 5 secs, took 0.20 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92049)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=92048)\u001b[0;0m INFO 12-24 06:01:14 model_runner.py:1527] Graph capturing finished in 5 secs, took 0.20 GiB\n",
      "INFO 12-24 06:01:14 model_runner.py:1527] Graph capturing finished in 5 secs, took 0.20 GiB\n",
      "INFO 12-24 06:01:14 llm_engine.py:446] init engine (profile, create kv cache, warmup model) took 15.45 seconds\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=model_id, max_model_len=4096, tensor_parallel_size=NUM_GPUS, gpu_memory_utilization=0.98, enable_chunked_prefill=True, max_num_batched_tokens=4096, max_num_seqs=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be81ccc6-6107-4b54-bf44-5749ea2d684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_content(topics):\n",
    "    messages = [[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": \"Topic:\\n\" + topic + \"\\n\\nContent:\"}] for topic in topics]\n",
    "    prompts = [tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True) for chat in messages]\n",
    "\n",
    "    outputs = llm.generate(prompts, SamplingParams(temperature=0.25, top_p=0.9, max_tokens=3072))\n",
    "\n",
    "    return {\"synthetic_content\": [output.outputs[0].text.strip() for output in outputs]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93f11b25-3337-43bd-9441-83034a216d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_ds_stream = ds.shuffle(seed=1998, buffer_size=1000_000).take(NUM_TOPICS).map(generate_content, batched=True, batch_size=NUM_TOPICS, input_columns=[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d47def01-041d-4d8a-9ae3-be4202400638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 1024/1024 [16:59<00:00,  1.00it/s, est. speed input: 777.05 toks/s, output: 468.71 toks/s]\n"
     ]
    }
   ],
   "source": [
    "syn_ds_list = list(syn_ds_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1eb5065-f94d-42a7-a143-cc547fd24895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60719cf32441437ebb477d2d09d8e161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08de17421254397a510db500680b63c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af02802a5b9c41998c9746cccc24cc43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/amang1802/wiki_topic_conditioned_405B/commit/b17e47bcaf7bbe4f3a329b6b3322599c72172036', commit_message='Upload dataset', commit_description='', oid='b17e47bcaf7bbe4f3a329b6b3322599c72172036', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/amang1802/wiki_topic_conditioned_405B', endpoint='https://huggingface.co', repo_type='dataset', repo_id='amang1802/wiki_topic_conditioned_405B'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_ds = Dataset.from_list(syn_ds_list)\n",
    "syn_ds.push_to_hub('amang1802/wiki_topic_conditioned_405B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daeea6a-a626-46c7-b0fc-5f478aecabef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
