{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbcf6e57-1c59-44c3-8710-a3fa927b4046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "from jinja2 import Template\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0107fb4c-5a46-4dbb-8774-209b71168555",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OUTPUTS = 1024 * 10\n",
    "NUM_GPUS = 4\n",
    "BATCH_SIZE = 256\n",
    "PUSH_INTERVAL = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8196360-e59b-4ce2-bbcd-c012f0ae1405",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Your goal is to create high quality learning content, like Wikipedia articles, that provides information collected through experience and research.\"\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5078a1c4-c45f-4ff8-9c80-a0e6b96a5570",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.1-405B-Instruct-FP8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8a56b4b-5c8c-4f2f-ac1f-9c68337d7154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-26 20:45:32 config.py:478] This model supports multiple tasks: {'classify', 'generate', 'reward', 'score', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 12-26 20:45:32 config.py:1216] Defaulting to use mp for distributed inference\n",
      "INFO 12-26 20:45:32 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='meta-llama/Llama-3.1-405B-Instruct-FP8', speculative_config=None, tokenizer='meta-llama/Llama-3.1-405B-Instruct-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fbgemm_fp8, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-405B-Instruct-FP8, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 12-26 20:45:33 multiproc_worker_utils.py:312] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 12-26 20:45:33 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 12-26 20:45:33 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52320)\u001b[0;0m INFO 12-26 20:45:33 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52320)\u001b[0;0m INFO 12-26 20:45:33 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52321)\u001b[0;0m INFO 12-26 20:45:33 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52321)\u001b[0;0m INFO 12-26 20:45:33 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52322)\u001b[0;0m INFO 12-26 20:45:33 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52322)\u001b[0;0m INFO 12-26 20:45:33 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "INFO 12-26 20:45:36 utils.py:922] Found nccl from library libnccl.so.2\n",
      "INFO 12-26 20:45:36 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52320)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=52321)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=52322)\u001b[0;0m INFO 12-26 20:45:36 utils.py:922] Found nccl from library libnccl.so.2\n",
      "INFO 12-26 20:45:36 utils.py:922] Found nccl from library libnccl.so.2\n",
      "INFO 12-26 20:45:36 utils.py:922] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52320)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=52322)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=52321)\u001b[0;0m INFO 12-26 20:45:36 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-26 20:45:36 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-26 20:45:36 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-26 20:45:38 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52321)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=52320)\u001b[0;0m INFO 12-26 20:45:38 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 12-26 20:45:38 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52322)\u001b[0;0m INFO 12-26 20:45:38 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 12-26 20:45:38 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_c4ef9bb5'), local_subscribe_port=44655, remote_subscribe_port=None)\n",
      "INFO 12-26 20:45:38 model_runner.py:1092] Starting to load model meta-llama/Llama-3.1-405B-Instruct-FP8...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52320)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=52322)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=52321)\u001b[0;0m INFO 12-26 20:45:38 model_runner.py:1092] Starting to load model meta-llama/Llama-3.1-405B-Instruct-FP8...\n",
      "INFO 12-26 20:45:38 model_runner.py:1092] Starting to load model meta-llama/Llama-3.1-405B-Instruct-FP8...\n",
      "INFO 12-26 20:45:38 model_runner.py:1092] Starting to load model meta-llama/Llama-3.1-405B-Instruct-FP8...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52320)\u001b[0;0m INFO 12-26 20:45:38 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 12-26 20:45:38 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52321)\u001b[0;0m INFO 12-26 20:45:38 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52322)\u001b[0;0m INFO 12-26 20:45:38 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41792dff13864256b26512a622c220b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/109 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=52322)\u001b[0;0m INFO 12-26 20:47:38 model_runner.py:1097] Loading model weights took 113.4847 GB\n",
      "INFO 12-26 20:47:38 model_runner.py:1097] Loading model weights took 113.4847 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52321)\u001b[0;0m INFO 12-26 20:47:38 model_runner.py:1097] Loading model weights took 113.4847 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52320)\u001b[0;0m INFO 12-26 20:47:38 model_runner.py:1097] Loading model weights took 113.4847 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52321)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=52320)\u001b[0;0m INFO 12-26 20:47:42 worker.py:241] Memory profiling takes 3.74 seconds\n",
      "INFO 12-26 20:47:42 worker.py:241] Memory profiling takes 3.74 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52321)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=52320)\u001b[0;0m INFO 12-26 20:47:42 worker.py:241] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.98) = 136.92GiB\n",
      "INFO 12-26 20:47:42 worker.py:241] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.98) = 136.92GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52321)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=52320)\u001b[0;0m INFO 12-26 20:47:42 worker.py:241] model weights take 113.48GiB; non_torch_memory takes 4.15GiB; PyTorch activation peak memory takes 0.76GiB; the rest of the memory reserved for KV Cache is 18.53GiB.\n",
      "INFO 12-26 20:47:42 worker.py:241] model weights take 113.48GiB; non_torch_memory takes 4.15GiB; PyTorch activation peak memory takes 0.76GiB; the rest of the memory reserved for KV Cache is 18.53GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52322)\u001b[0;0m INFO 12-26 20:47:42 worker.py:241] Memory profiling takes 3.75 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52322)\u001b[0;0m INFO 12-26 20:47:42 worker.py:241] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.98) = 136.92GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52322)\u001b[0;0m INFO 12-26 20:47:42 worker.py:241] model weights take 113.48GiB; non_torch_memory takes 3.68GiB; PyTorch activation peak memory takes 0.76GiB; the rest of the memory reserved for KV Cache is 19.00GiB.\n",
      "INFO 12-26 20:47:42 worker.py:241] Memory profiling takes 3.79 seconds\n",
      "INFO 12-26 20:47:42 worker.py:241] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.98) = 136.92GiB\n",
      "INFO 12-26 20:47:42 worker.py:241] model weights take 113.48GiB; non_torch_memory takes 4.93GiB; PyTorch activation peak memory takes 1.32GiB; the rest of the memory reserved for KV Cache is 17.19GiB.\n",
      "INFO 12-26 20:47:42 distributed_gpu_executor.py:57] # GPU blocks: 8939, # CPU blocks: 2080\n",
      "INFO 12-26 20:47:42 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 34.92x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52320)\u001b[0;0m INFO 12-26 20:47:46 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52320)\u001b[0;0m INFO 12-26 20:47:46 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-26 20:47:46 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-26 20:47:46 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52322)\u001b[0;0m INFO 12-26 20:47:46 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52322)\u001b[0;0m INFO 12-26 20:47:46 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52321)\u001b[0;0m INFO 12-26 20:47:46 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52321)\u001b[0;0m INFO 12-26 20:47:46 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52322)\u001b[0;0m INFO 12-26 20:48:04 custom_all_reduce.py:224] Registering 8602 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52320)\u001b[0;0m INFO 12-26 20:48:05 custom_all_reduce.py:224] Registering 8602 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52321)\u001b[0;0m INFO 12-26 20:48:05 custom_all_reduce.py:224] Registering 8602 cuda graph addresses\n",
      "INFO 12-26 20:48:05 custom_all_reduce.py:224] Registering 8602 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52322)\u001b[0;0m INFO 12-26 20:48:05 model_runner.py:1527] Graph capturing finished in 19 secs, took 0.78 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52320)\u001b[0;0m INFO 12-26 20:48:05 model_runner.py:1527] Graph capturing finished in 19 secs, took 0.78 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=52321)\u001b[0;0m INFO 12-26 20:48:05 model_runner.py:1527] Graph capturing finished in 19 secs, took 0.77 GiB\n",
      "INFO 12-26 20:48:05 model_runner.py:1527] Graph capturing finished in 19 secs, took 0.78 GiB\n",
      "INFO 12-26 20:48:05 llm_engine.py:446] init engine (profile, create kv cache, warmup model) took 27.12 seconds\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=model_id, max_model_len=4096, tensor_parallel_size=NUM_GPUS, gpu_memory_utilization=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be81ccc6-6107-4b54-bf44-5749ea2d684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_content(n):\n",
    "    message = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "               {\"role\": \"user\", \"content\": \"Write a long article.\\n\\nTitle:\"}]\n",
    "\n",
    "    outputs = llm.chat([message]*n, SamplingParams(temperature=1, top_p=0.9, max_tokens=3584))\n",
    "\n",
    "    return {\"synthetic_content\": [output.outputs[0].text.strip() for output in outputs]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b3fc99a-3df2-472f-a135-f68e35f9233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_content_base(n):\n",
    "    prompt = \"Here is long high quality learning article, like a Wikipedia article, that provides information collected through experience and research. The article is formatted for easy readibility.\\n\\nTitle:\"\n",
    "    outputs = llm.generate([prompt]*n, SamplingParams(temperature=0.75, top_p=0.9, max_tokens=4096))\n",
    "\n",
    "    return {\"synthetic_content\": [output.outputs[0].text.strip() for output in outputs]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6daeea6a-a626-46c7-b0fc-5f478aecabef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-26 20:50:51 chat_utils.py:333] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 4/4 [00:57<00:00, 14.28s/it, est. speed input: 4.62 toks/s, output: 82.00 toks/s]\n"
     ]
    }
   ],
   "source": [
    "ds = Dataset.from_dict(generate_content(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b70bc90-a3ae-46e5-a11c-4687e38b9562",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_id = \"amang1802/synthetic_data_unconditioned_L3.1_405B_Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "563e902d-1480-4605-a58a-376270f7c1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   1% 3/256 [00:54<54:35, 12.95s/it, est. speed input: 1.86 toks/s, output: 27.11 toks/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 20:54:06 scheduler.py:1555] Sequence group 259 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [03:58<00:00,  1.07it/s, est. speed input: 36.50 toks/s, output: 893.19 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   3% 8/256 [00:58<08:54,  2.16s/it, est. speed input: 4.67 toks/s, output: 72.40 toks/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 20:58:07 scheduler.py:1555] Sequence group 509 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [03:35<00:00,  1.19it/s, est. speed input: 40.34 toks/s, output: 998.10 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   1% 3/256 [00:59<58:32, 13.88s/it, est. speed input: 1.72 toks/s, output: 27.01 toks/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 21:01:44 scheduler.py:1555] Sequence group 757 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [03:54<00:00,  1.09it/s, est. speed input: 37.15 toks/s, output: 899.04 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  11% 29/256 [01:08<01:20,  2.81it/s, est. speed input: 14.47 toks/s, output: 241.82 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 21:05:48 scheduler.py:1555] Sequence group 1011 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [02:25<00:00,  1.76it/s, est. speed input: 59.75 toks/s, output: 1451.98 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  12% 32/256 [01:11<01:24,  2.64it/s, est. speed input: 15.19 toks/s, output: 269.25 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 21:08:17 scheduler.py:1555] Sequence group 1263 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [02:11<00:00,  1.95it/s, est. speed input: 66.41 toks/s, output: 1586.35 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [03:55<00:00,  1.09it/s, est. speed input: 36.98 toks/s, output: 895.12 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   4% 10/256 [00:56<05:22,  1.31s/it, est. speed input: 5.99 toks/s, output: 84.33 toks/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 21:14:09 scheduler.py:1555] Sequence group 1792 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [02:15<00:00,  1.88it/s, est. speed input: 64.03 toks/s, output: 1521.22 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   5% 12/256 [01:01<03:32,  1.15it/s, est. speed input: 6.61 toks/s, output: 104.89 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 21:16:29 scheduler.py:1555] Sequence group 2039 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [01:55<00:00,  2.21it/s, est. speed input: 75.14 toks/s, output: 1773.61 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 2052\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d9d8e2fd1a04e0f949376c0c6f51b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a9f987b9534cfb8b85b588802f1c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45079b058e454f6c8d8b897740b614cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   6% 15/256 [01:04<03:51,  1.04it/s, est. speed input: 7.94 toks/s, output: 118.99 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 21:18:29 scheduler.py:1555] Sequence group 2284 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [03:56<00:00,  1.08it/s, est. speed input: 36.88 toks/s, output: 894.19 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 2308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   5% 13/256 [01:06<04:00,  1.01it/s, est. speed input: 6.68 toks/s, output: 112.62 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 21:22:28 scheduler.py:1555] Sequence group 2535 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [03:58<00:00,  1.07it/s, est. speed input: 36.51 toks/s, output: 896.70 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 2564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   9% 24/256 [01:09<01:54,  2.03it/s, est. speed input: 11.81 toks/s, output: 202.47 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 21:26:28 scheduler.py:1555] Sequence group 2796 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [02:21<00:00,  1.81it/s, est. speed input: 61.46 toks/s, output: 1475.26 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 2820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  14% 35/256 [01:13<02:08,  1.72it/s, est. speed input: 16.10 toks/s, output: 286.28 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 21:28:55 scheduler.py:1555] Sequence group 3055 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [03:57<00:00,  1.08it/s, est. speed input: 36.63 toks/s, output: 904.05 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 3076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [02:08<00:00,  1.99it/s, est. speed input: 67.70 toks/s, output: 1636.06 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 3332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   2% 4/256 [00:55<43:13, 10.29s/it, est. speed input: 2.46 toks/s, output: 28.91 toks/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 21:34:43 scheduler.py:1555] Sequence group 3584 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [02:14<00:00,  1.91it/s, est. speed input: 64.94 toks/s, output: 1529.44 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 3588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0% 1/256 [00:52<3:42:34, 52.37s/it, est. speed input: 0.65 toks/s, output: 9.30 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 21:36:59 scheduler.py:1555] Sequence group 3830 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [04:01<00:00,  1.06it/s, est. speed input: 36.03 toks/s, output: 901.30 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 3844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   7% 18/256 [01:04<02:55,  1.36it/s, est. speed input: 9.52 toks/s, output: 152.04 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 21:41:07 scheduler.py:1555] Sequence group 4089 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [02:22<00:00,  1.79it/s, est. speed input: 60.91 toks/s, output: 1467.10 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 4100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0162a85899f647cdac8f035a80e27606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d91bab33ea74804982edcef7798aafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be85c8de8adb4b47ae53c56c3dbfddb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0a85733f1f4b9e91d4d3714ac13069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   3% 8/256 [01:02<10:38,  2.58s/it, est. speed input: 4.36 toks/s, output: 68.88 toks/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 21:43:29 scheduler.py:1555] Sequence group 4330 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [02:11<00:00,  1.95it/s, est. speed input: 66.43 toks/s, output: 1616.81 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 4356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  12% 32/256 [01:09<01:44,  2.15it/s, est. speed input: 15.65 toks/s, output: 261.44 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 21:45:48 scheduler.py:1555] Sequence group 4596 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [03:56<00:00,  1.08it/s, est. speed input: 36.79 toks/s, output: 904.53 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 4612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  14% 37/256 [01:13<01:07,  3.24it/s, est. speed input: 17.17 toks/s, output: 308.29 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 21:49:48 scheduler.py:1555] Sequence group 4848 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [03:57<00:00,  1.08it/s, est. speed input: 36.60 toks/s, output: 909.86 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 4868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [02:55<00:00,  1.46it/s, est. speed input: 49.73 toks/s, output: 1190.39 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 5124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   1% 3/256 [00:56<55:37, 13.19s/it, est. speed input: 1.81 toks/s, output: 27.07 toks/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 21:56:24 scheduler.py:1555] Sequence group 5377 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [02:31<00:00,  1.69it/s, est. speed input: 57.37 toks/s, output: 1383.83 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 5380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   4% 11/256 [00:59<04:47,  1.17s/it, est. speed input: 6.26 toks/s, output: 97.95 toks/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 21:58:59 scheduler.py:1555] Sequence group 5628 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [04:01<00:00,  1.06it/s, est. speed input: 36.00 toks/s, output: 897.52 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 5636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   5% 14/256 [01:02<04:20,  1.08s/it, est. speed input: 7.61 toks/s, output: 122.77 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 22:03:05 scheduler.py:1555] Sequence group 5874 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [02:02<00:00,  2.10it/s, est. speed input: 71.34 toks/s, output: 1701.53 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 5892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  13% 34/256 [01:10<01:22,  2.70it/s, est. speed input: 16.33 toks/s, output: 281.81 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 22:05:14 scheduler.py:1555] Sequence group 6134 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [02:15<00:00,  1.89it/s, est. speed input: 64.31 toks/s, output: 1533.65 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 6148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a3621061eb453c82c797b658cc6883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b1f799fb8e45f787ec11d11c9ecc80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06fd435b9644652b83034dcc377900a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152d2c07e70c4b6190140c64aa6c1479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/289 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [02:05<00:00,  2.04it/s, est. speed input: 69.30 toks/s, output: 1678.36 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 6404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   1% 2/256 [00:56<1:44:35, 24.71s/it, est. speed input: 1.21 toks/s, output: 16.94 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 22:09:22 scheduler.py:1555] Sequence group 6658 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [03:54<00:00,  1.09it/s, est. speed input: 37.15 toks/s, output: 900.66 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 6660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   9% 24/256 [01:05<01:32,  2.50it/s, est. speed input: 12.51 toks/s, output: 202.76 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 22:13:25 scheduler.py:1555] Sequence group 6908 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [03:53<00:00,  1.10it/s, est. speed input: 37.28 toks/s, output: 904.40 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 6916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [02:06<00:00,  2.03it/s, est. speed input: 68.90 toks/s, output: 1639.46 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 7172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   2% 4/256 [00:54<32:14,  7.68s/it, est. speed input: 2.50 toks/s, output: 36.38 toks/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 22:19:16 scheduler.py:1555] Sequence group 7425 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [02:11<00:00,  1.95it/s, est. speed input: 66.23 toks/s, output: 1572.58 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 7428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   1% 3/256 [00:58<56:28, 13.39s/it, est. speed input: 1.75 toks/s, output: 27.43 toks/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 22:21:30 scheduler.py:1555] Sequence group 7669 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [01:57<00:00,  2.17it/s, est. speed input: 73.81 toks/s, output: 1757.66 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 7684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   7% 18/256 [01:04<03:43,  1.07it/s, est. speed input: 9.55 toks/s, output: 149.27 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 22:23:34 scheduler.py:1555] Sequence group 7921 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [02:13<00:00,  1.92it/s, est. speed input: 65.22 toks/s, output: 1523.77 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 7940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  20% 52/256 [01:17<01:00,  3.35it/s, est. speed input: 22.96 toks/s, output: 416.63 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 22:26:00 scheduler.py:1555] Sequence group 8195 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [01:59<00:00,  2.14it/s, est. speed input: 72.74 toks/s, output: 1703.82 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 8196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e72de65df0d42b089f67914963e7475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ee39640ae6491a9da50222e8c8ffa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d22bdf10284da18f3c4062c9ba518c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/lfs.py:337: UserWarning: hf_transfer is enabled but does not support uploading from bytes or BinaryIO, falling back to regular upload\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c3ab5669364ce2b4d3ce2601ec2595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/290 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  24% 61/256 [01:22<01:14,  2.63it/s, est. speed input: 24.99 toks/s, output: 470.64 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 22:28:07 scheduler.py:1555] Sequence group 8447 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [03:58<00:00,  1.07it/s, est. speed input: 36.51 toks/s, output: 895.37 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 8452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [02:40<00:00,  1.60it/s, est. speed input: 54.24 toks/s, output: 1287.31 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 8708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   2% 5/256 [00:58<25:56,  6.20s/it, est. speed input: 2.93 toks/s, output: 43.27 toks/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 22:34:20 scheduler.py:1555] Sequence group 8956 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [03:55<00:00,  1.09it/s, est. speed input: 36.98 toks/s, output: 881.96 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 8964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   5% 13/256 [01:02<03:19,  1.22it/s, est. speed input: 7.03 toks/s, output: 113.66 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 22:38:21 scheduler.py:1555] Sequence group 9201 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [03:56<00:00,  1.08it/s, est. speed input: 36.73 toks/s, output: 902.87 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 9220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  12% 30/256 [01:09<01:47,  2.10it/s, est. speed input: 14.71 toks/s, output: 248.89 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 22:42:25 scheduler.py:1555] Sequence group 9457 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [02:11<00:00,  1.95it/s, est. speed input: 66.28 toks/s, output: 1582.58 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 9476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   9% 22/256 [01:07<02:00,  1.94it/s, est. speed input: 11.08 toks/s, output: 187.70 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 22:44:34 scheduler.py:1555] Sequence group 9707 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [02:00<00:00,  2.13it/s, est. speed input: 72.42 toks/s, output: 1744.89 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 9732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  12% 32/256 [01:11<01:31,  2.44it/s, est. speed input: 15.14 toks/s, output: 269.47 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-26 22:46:39 scheduler.py:1555] Sequence group 9969 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [03:56<00:00,  1.08it/s, est. speed input: 36.78 toks/s, output: 895.79 toks/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 9988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100% 256/256 [03:55<00:00,  1.09it/s, est. speed input: 36.99 toks/s, output: 898.59 toks/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 10244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b634559795ad4eb6b74685d0f6956b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c790b67cf3a4343b13871d04a384fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75622368bd74012a8f981c9bdf69228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f24dd39440b14559a31e9302e502a326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/290 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_batches = NUM_OUTPUTS // BATCH_SIZE\n",
    "for i in range(num_batches):\n",
    "    new_data = Dataset.from_dict(generate_content_base(BATCH_SIZE))\n",
    "    ds = concatenate_datasets([ds, new_data])\n",
    "    print(f\"Dataset size: {ds.num_rows}\")\n",
    "    if (i+1) % PUSH_INTERVAL == 0:\n",
    "        ds.push_to_hub(ds_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c180cd-4a82-4fe0-90e2-d7e2a2a225f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
