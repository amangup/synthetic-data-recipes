{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbcf6e57-1c59-44c3-8710-a3fa927b4046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from jinja2 import Template\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0107fb4c-5a46-4dbb-8774-209b71168555",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GPUS = 4\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a9d9a50-7876-49d5-8f18-27558e4584c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"topic_conditioned.jinja2\") as f:\n",
    "    template_str = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3379682b-e945-48ce-bfea-b689076cb0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"topic_content_shots.json\") as f:\n",
    "    content_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79c11090-d732-4015-9040-644c7ff0f0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = Template(template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8196360-e59b-4ce2-bbcd-c012f0ae1405",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = template.render(contents=content_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d3cb5f4-7420-473f-9cfc-a0aa249c12f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is long high quality learning article, like a Wikipedia article, that provides information collected through experience and research. The article is formatted for easy readibility.\n",
      "\n",
      "Title: **Gordon Ramsay's early career**\n",
      "Gordon James Ramsay was born in Johnstone, Scotland, on 8 November 1966, the son of Helen (née Cosgrove), a nurse, and Gordon James Sr., who worked as a swimming pool manager, welder, and shopkeeper. He has an older sister, a younger brother, and a younger sister. When he was nine years old, he moved with his family to England and grew up in the Bishopton area of Stratford-upon-Avon. He has described his early life as 'hopelessly itinerant' and said his family moved constantly owing to the aspirations and failures of his father, who was an occasionally violent alcoholic; Ramsay described him as a 'hard-drinking womaniser'. In his autobiography, he revealed that his father abused and neglected the children and his mother. He worked as a pot washer in a local Indian restaurant where his sister was a waitress. He had hoped to become a footballer and was first chosen to play under-14 football at the age of 12, but his early footballing career was marked by injuries; after a serious knee injury, he was forced to give it up. At the age of 16, he moved out of the family home and into an apartment in Banbury.\n",
      "Ramsay's interest in cooking began in his teenage years; rather than be known as 'the football player with the gammy knee', he decided to pay more serious attention to his culinary education at age 19. Ramsay enrolled at North Oxfordshire Technical College, sponsored by the Rotarians, to study hotel management. He described his decision to enter catering college as 'a complete accident'.\n",
      "In the mid-1980s, Ramsay worked as a commis chef at the Wroxton House Hotel. He ran the kitchen and 60-seat dining room at the Wickham Arms until he quit after having sex with the owner's wife. Ramsay then moved to London, where he worked in a series of restaurants until being inspired to work for Marco Pierre White at Harveys.\n",
      "After working at Harveys for two years and ten months, Ramsay, tired of 'the rages and the bullying and violence', decided that the way to further advance his career was to study French cuisine. White discouraged Ramsay from taking a job in Paris, instead encouraging him to work for Albert Roux at Le Gavroche in Mayfair. Ramsay decided to take his advice, and there, Ramsay met Jean-Claude Breton, who later became his maître d'hôtel at Restaurant Gordon Ramsay. After Ramsay worked at Le Gavroche for a year, Roux invited him to work with him at Hotel Diva, a ski resort in the French Alps, as his number two. From there, a 23-year-old Ramsay moved to Paris to work with Guy Savoy and Joël Robuchon, both Michelin-starred chefs. He continued his training in France for three years, before giving in to the physical and mental stress of the kitchens and taking a year to work as a personal chef on the private yacht Idlewild, based in Bermuda. The role on the boat saw him travel to Sicily and Sardinia, Italy, and learn about Italian cuisine.\n",
      "\n",
      "Title: **The Coriolis Force**\n",
      "I am often asked by students whether their toilet bowl will flush clockwise or counterclockwise in the southern hemisphere, or whether it will flush straight down in Ecuador. This would, of course, be important information if you were ever kidnapped and blindfolded and dropped off in a strange land. If we assume a commode of conventional size, then this “toilet bowl test” will fail because the answer lies in the manufacturer’s design. But if your northern hemisphere toilet bowl were a few hundred miles in diameter then the Coriolis force of the rotating Earth would easily overcome the random water currents, and force the bowl to empty its contents in a counter clockwise swirl. If you have southern hemisphere friends with an equally large toilet, then theirs would indeed empty in the opposite (clockwise) direction.\n",
      "The circulation within oversized flush toilets is a natural consequence of motion on the surface of an object that rotates. We owe our detailed understanding of the effect to the work of Gaspard Gustave de Coriolis who, in 1831, presented details of the laws of mechanics in a rotating reference frame to the Academie des Sciences in Paris. Earth’s surface provides an excellent place to demonstrate why the origin of the Coriolis force is relatively simple. Our planet rotates on its axis approximately once every 24 hours. In that 24 hour period, objects on Earth’s equator travel a circle with a circumference of nearly 25,000 miles, which corresponds to a speed of a more than 1,000 miles per hour. By 41° north, the latitude of New York City and the American Museum of Natural History, the circumference traveled is only about 19,000 miles. The west-to-east speed is now approximately 800 miles per hour. As you continue to increase in Earth latitude (north or south of the equator) your west-to-east speed decreases until it hits exactly zero miles per hour at the poles. (For this reason, most satellites are launched as close to the equator as possible, which enables them to get a good “running start” in their eastward orbits.\n",
      "Imagine a puffy cloud in the northern hemisphere and a meteorological low pressure system directly to its north. The cloud will tend to move toward the low. But during the journey its greater eastward speed will enable the cloud to overtake the low, which is itself in motion, and end up east of its destination. Another puffy cloud that is north of the low will also tend to move toward the low, but will naturally lag behind and end up west of its destination. To an unsuspecting person on Earth’s surface, these curved north-south paths would appear to be the effects of a mysterious force (the Coriolis force) yet no true force was ever at work.\n",
      "When many puffy clouds approach a low pressure system from all directions you get a merry-go-round of counter-clockwise motion, which is better known as a cyclone. In extreme cases you get a monstrous hurricane with wind speeds upwards of a hundred miles per hour. For the southern hemisphere the same arguments will create a cyclone that spirals clockwise. The military normally knows all about the Coriolis force and thus introduces the appropriate correction to all missile trajectories. But in 1914, from the annals of embarrassing military moments, there was a World War I naval battle between the English and the Germans near the Falklands Islands off Argentina (52° south latitude). The English battle cruisers Invincible and Inflexible engaged the German war ships Gneisenau and Scharnhorst at a range of nearly ten miles. Among other gunnery problems encountered, the English forgot to reverse the direction of their Coriolis correction. Their tables had been calculated for northern hemisphere projectiles, so they missed their targets by even more than if no correction had been applied. They ultimately won the battle against the Germans with about sixty direct hits, but it was not before over a thousand missile shells had fallen in the ocean.\n",
      "In high school I knew all about the Coriolis force, but I never had the opportunity to test it on something as large as a swimming pool until the summer after my junior year when I worked as a lifeguard. At the mid-summer cleaning, I opened the drain valve to the pool and carefully observed the circulation. The water funneled in the “wrong” direction—clockwise. The last I had checked, I was life-guarding in Earth’s northern hemisphere so I was tempted to declare Coriolis forces to be a hoax. But a fast “back of the envelope” calculation verified that the difference in Coriolis velocity across the pool was a mere ½ inch per minute. This is slow. The water currents from somebody just climbing out of the pool, or even a gentle breeze across the water’s surface would easily swamp the effect and I would end up with clockwise one half the time and counterclockwise the other half of the time. A proper experiment to demonstrate the insignificance of the Coriolis forces would require that I empty and refill the pool dozens of times. But each try would dump 15,000 cubic feet of water and diminish my job security. So I didn’t.\n",
      "The air circulation near a high pressure systems, which are inelegantly known as anticyclones, is a reverse picture of our cyclone. On Earth, these high pressure systems are the astronomer’s best friend because they are typically devoid of clouds. The surrounding air still circulates, but it does so without the benefit of clouds as tracers of the air flow. The circulation around low and high pressure systems, known as geostrophic winds, presents us with the paradox that Coriolis forces tend to move air along lines of constant pressure (isobars) rather than across them.\n",
      "Now imagine, if you will, a place that is not only fourteen hundred times larger than Earth, but has an equatorial speed that is about twenty-five times as fast, and has a deep, thick, colorful atmosphere. That place is the planet Jupiter, where a day lasts just 9 hours and 56 minutes. It is a cosmic garden of atmospheric dynamics where all rotationally induced cloud and weather patterns are correspondingly enhanced. In the most striking display of the Coriolis force in the entire solar system, Jupiter lays claim to the largest, most energetic, and longest-lived storm ever observed. It is an anticyclone that looks like a great red spot in Jupiter’s upper atmosphere. We call it Jupiter’s Great Red Spot. Discovered in the mid 1660s by the English physicist Robert Hooke and separately by the Italian astronomer Giovanni Cassini, the feature has persisted for over 300 years. It was not until the twentieth century when the Dutch-born, American astronomer Gerard Kuiper was the first to supply the modern interpretation of the Spot as a raging storm.\n",
      "\n",
      "Title: **Passing of Rep. John Lewis**\n",
      "America is a constant work in progress. What gives each new generation purpose is to take up the unfinished work of the last and carry it further — to speak out for what’s right, to challenge an unjust status quo, and to imagine a better world.\n",
      "John Lewis — one of the original Freedom Riders, chairman of the Student Nonviolent Coordinating Committee, the youngest speaker at the March on Washington, leader of the march from Selma to Montgomery, Member of Congress representing the people of Georgia for 33 years — not only assumed that responsibility, he made it his life’s work. He loved this country so much that he risked his life and his blood so that it might live up to its promise. And through the decades, he not only gave all of himself to the cause of freedom and justice, but inspired generations that followed to try to live up to his example.\n",
      "Considering his enormous impact on the history of this country, what always struck those who met John was his gentleness and humility. Born into modest means in the heart of the Jim Crow South, he understood that he was just one of a long line of heroes in the struggle for racial justice. Early on, he embraced the principles of nonviolent resistance and civil disobedience as the means to bring about real change in this country, understanding that such tactics had the power not only to change laws, but to change hearts and minds as well.\n",
      "In so many ways, John’s life was exceptional. But he never believed that what he did was more than any citizen of this country might do. He believed that in all of us, there exists the capacity for great courage, a longing to do what’s right, a willingness to love all people, and to extend to them their God-given rights to dignity and respect. And it’s because he saw the best in all of us that he will continue, even in his passing, to serve as a beacon in that long journey towards a more perfect union.\n",
      "I first met John when I was in law school, and I told him then that he was one of my heroes. Years later, when I was elected a U.S. Senator, I told him that I stood on his shoulders. When I was elected President of the United States, I hugged him on the inauguration stand before I was sworn in and told him I was only there because of the sacrifices he made. And through all those years, he never stopped providing wisdom and encouragement to me and Michelle and our family. We will miss him dearly.\n",
      "It’s fitting that the last time John and I shared a public forum was at a virtual town hall with a gathering of young activists who were helping to lead this summer’s demonstrations in the wake of George Floyd’s death. Afterwards, I spoke to him privately, and he could not have been prouder of their efforts — of a new generation standing up for freedom and equality, a new generation intent on voting and protecting the right to vote, a new generation running for political office. I told him that all those young people — of every race, from every background and gender and sexual orientation — they were his children. They had learned from his example, even if they didn’t know it. They had understood through him what American citizenship requires, even if they had heard of his courage only through history books.\n",
      "Not many of us get to live to see our own legacy play out in such a meaningful, remarkable way. John Lewis did. And thanks to him, we now all have our marching orders — to keep believing in the possibility of remaking this country we love until it lives up to its full promise.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2fdeca6-d6b4-4d2f-a438-6394da58945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('amang1802/synthetic_data_qna_fulltext_conditioned_L3.3_70B_deduped')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5078a1c4-c45f-4ff8-9c80-a0e6b96a5570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_id = \"/root/synthetic-data-recipes/cpt/ft_models/llama3_1_8B/qna_fulltext_conditioned_20epochs_lr1e-5/epoch_19\"\n",
    "model_id = \"meta-llama/Llama-3.1-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8a56b4b-5c8c-4f2f-ac1f-9c68337d7154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-06 19:08:19 config.py:478] This model supports multiple tasks: {'embed', 'classify', 'score', 'generate', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 01-06 19:08:19 config.py:1216] Defaulting to use mp for distributed inference\n",
      "INFO 01-06 19:08:19 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='meta-llama/Llama-3.1-8B', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=6144, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 01-06 19:08:19 multiproc_worker_utils.py:312] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 01-06 19:08:19 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 01-06 19:08:20 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552739)\u001b[0;0m INFO 01-06 19:08:20 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552739)\u001b[0;0m INFO 01-06 19:08:20 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552738)\u001b[0;0m INFO 01-06 19:08:20 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552738)\u001b[0;0m INFO 01-06 19:08:20 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552740)\u001b[0;0m INFO 01-06 19:08:20 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552740)\u001b[0;0m INFO 01-06 19:08:20 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "INFO 01-06 19:08:23 utils.py:922] Found nccl from library libnccl.so.2\n",
      "INFO 01-06 19:08:23 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552739)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=552738)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=552740)\u001b[0;0m INFO 01-06 19:08:23 utils.py:922] Found nccl from library libnccl.so.2\n",
      "INFO 01-06 19:08:23 utils.py:922] Found nccl from library libnccl.so.2\n",
      "INFO 01-06 19:08:23 utils.py:922] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552739)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=552738)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=552740)\u001b[0;0m INFO 01-06 19:08:23 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 01-06 19:08:23 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 01-06 19:08:23 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 01-06 19:08:25 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552739)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=552738)\u001b[0;0m INFO 01-06 19:08:25 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552740)\u001b[0;0m INFO 01-06 19:08:25 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 01-06 19:08:25 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 01-06 19:08:25 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_839c0f5b'), local_subscribe_port=49055, remote_subscribe_port=None)\n",
      "INFO 01-06 19:08:25 model_runner.py:1092] Starting to load model meta-llama/Llama-3.1-8B...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552739)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=552738)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=552740)\u001b[0;0m INFO 01-06 19:08:25 model_runner.py:1092] Starting to load model meta-llama/Llama-3.1-8B...\n",
      "INFO 01-06 19:08:25 model_runner.py:1092] Starting to load model meta-llama/Llama-3.1-8B...\n",
      "INFO 01-06 19:08:25 model_runner.py:1092] Starting to load model meta-llama/Llama-3.1-8B...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552739)\u001b[0;0m INFO 01-06 19:08:25 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552738)\u001b[0;0m INFO 01-06 19:08:25 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 01-06 19:08:25 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552740)\u001b[0;0m INFO 01-06 19:08:25 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b65d49a82546c899b601fd73d9a965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=552739)\u001b[0;0m INFO 01-06 19:08:26 model_runner.py:1097] Loading model weights took 3.7710 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552738)\u001b[0;0m INFO 01-06 19:08:27 model_runner.py:1097] Loading model weights took 3.7710 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552740)\u001b[0;0m INFO 01-06 19:08:27 model_runner.py:1097] Loading model weights took 3.7710 GB\n",
      "INFO 01-06 19:08:27 model_runner.py:1097] Loading model weights took 3.7710 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552739)\u001b[0;0m INFO 01-06 19:08:30 worker.py:241] Memory profiling takes 2.66 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552739)\u001b[0;0m INFO 01-06 19:08:30 worker.py:241] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.98) = 136.92GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552739)\u001b[0;0m INFO 01-06 19:08:30 worker.py:241] model weights take 3.77GiB; non_torch_memory takes 4.16GiB; PyTorch activation peak memory takes 0.31GiB; the rest of the memory reserved for KV Cache is 128.69GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552738)\u001b[0;0m INFO 01-06 19:08:30 worker.py:241] Memory profiling takes 2.67 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552740)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=552738)\u001b[0;0m INFO 01-06 19:08:30 worker.py:241] Memory profiling takes 2.67 seconds\n",
      "INFO 01-06 19:08:30 worker.py:241] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.98) = 136.92GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552740)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=552738)\u001b[0;0m INFO 01-06 19:08:30 worker.py:241] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.98) = 136.92GiB\n",
      "INFO 01-06 19:08:30 worker.py:241] model weights take 3.77GiB; non_torch_memory takes 4.16GiB; PyTorch activation peak memory takes 0.31GiB; the rest of the memory reserved for KV Cache is 128.69GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552740)\u001b[0;0m INFO 01-06 19:08:30 worker.py:241] model weights take 3.77GiB; non_torch_memory takes 3.69GiB; PyTorch activation peak memory takes 0.31GiB; the rest of the memory reserved for KV Cache is 129.15GiB.\n",
      "INFO 01-06 19:08:30 worker.py:241] Memory profiling takes 2.69 seconds\n",
      "INFO 01-06 19:08:30 worker.py:241] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.98) = 136.92GiB\n",
      "INFO 01-06 19:08:30 worker.py:241] model weights take 3.77GiB; non_torch_memory takes 4.94GiB; PyTorch activation peak memory takes 1.24GiB; the rest of the memory reserved for KV Cache is 126.97GiB.\n",
      "INFO 01-06 19:08:30 distributed_gpu_executor.py:57] # GPU blocks: 260029, # CPU blocks: 8192\n",
      "INFO 01-06 19:08:30 distributed_gpu_executor.py:61] Maximum concurrency for 6144 tokens per request: 677.16x\n",
      "INFO 01-06 19:08:32 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-06 19:08:32 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552739)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=552738)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=552740)\u001b[0;0m INFO 01-06 19:08:32 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552739)\u001b[0;0m INFO 01-06 19:08:32 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-06 19:08:32 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-06 19:08:32 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552740)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=552738)\u001b[0;0m INFO 01-06 19:08:32 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 01-06 19:08:32 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 01-06 19:08:46 custom_all_reduce.py:224] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552739)\u001b[0;0m INFO 01-06 19:08:46 custom_all_reduce.py:224] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552738)\u001b[0;0m INFO 01-06 19:08:46 custom_all_reduce.py:224] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552740)\u001b[0;0m INFO 01-06 19:08:46 custom_all_reduce.py:224] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552740)\u001b[0;0m INFO 01-06 19:08:46 model_runner.py:1527] Graph capturing finished in 15 secs, took 0.23 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552738)\u001b[0;0m INFO 01-06 19:08:46 model_runner.py:1527] Graph capturing finished in 15 secs, took 0.23 GiB\n",
      "INFO 01-06 19:08:46 model_runner.py:1527] Graph capturing finished in 15 secs, took 0.23 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=552739)\u001b[0;0m INFO 01-06 19:08:46 model_runner.py:1527] Graph capturing finished in 15 secs, took 0.23 GiB\n",
      "INFO 01-06 19:08:46 llm_engine.py:446] init engine (profile, create kv cache, warmup model) took 19.48 seconds\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=model_id, max_model_len=6144, tensor_parallel_size=NUM_GPUS, gpu_memory_utilization=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be81ccc6-6107-4b54-bf44-5749ea2d684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_content_base(titles):\n",
    "    prompts = [f\"{system_prompt}\\n\\nTitle: **{title}**\\n\" for title in titles]\n",
    "    outputs = llm.generate(prompts, SamplingParams(temperature=0.25, top_p=0.9, max_tokens=2048, stop=[\"Title:\"]))\n",
    "\n",
    "    return {\"cpt_gen_content\": [f\"**{title}**\\n{output.outputs[0].text.strip()}\" for title, output in zip(titles, outputs)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f11b25-3337-43bd-9441-83034a216d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function generate_content_base at 0x7b081f1548b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bc0cff213ab421b9989c8f6b902f85e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Acessed prompts:   0% 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "\u001b[Acessed prompts:   0% 1/1024 [00:08<2:21:56,  8.33s/it, est. speed input: 344.61 toks/s, output: 3.36 toks/s]\n",
      "\u001b[Acessed prompts:   0% 2/1024 [00:08<1:03:25,  3.72s/it, est. speed input: 649.94 toks/s, output: 8.38 toks/s]\n",
      "\u001b[Acessed prompts:   0% 3/1024 [00:09<36:06,  2.12s/it, est. speed input: 951.46 toks/s, output: 14.04 toks/s] \n",
      "\u001b[Acessed prompts:   0% 4/1024 [00:09<22:35,  1.33s/it, est. speed input: 1253.33 toks/s, output: 19.99 toks/s]\n",
      "\u001b[Acessed prompts:   1% 6/1024 [00:09<13:13,  1.28it/s, est. speed input: 1766.56 toks/s, output: 32.54 toks/s]\n",
      "\u001b[Acessed prompts:   1% 7/1024 [00:09<10:07,  1.68it/s, est. speed input: 2036.87 toks/s, output: 40.18 toks/s]\n",
      "\u001b[Acessed prompts:   1% 9/1024 [00:10<07:52,  2.15it/s, est. speed input: 2470.40 toks/s, output: 55.12 toks/s]\n",
      "\u001b[Acessed prompts:   1% 10/1024 [00:10<06:26,  2.62it/s, est. speed input: 2715.35 toks/s, output: 64.18 toks/s]\n",
      "\u001b[Acessed prompts:   1% 12/1024 [00:10<05:06,  3.31it/s, est. speed input: 3146.99 toks/s, output: 81.73 toks/s]\n",
      "\u001b[Acessed prompts:   1% 13/1024 [00:11<06:03,  2.78it/s, est. speed input: 3243.80 toks/s, output: 89.42 toks/s]\n",
      "\u001b[Acessed prompts:   1% 14/1024 [00:11<05:48,  2.90it/s, est. speed input: 3405.53 toks/s, output: 99.38 toks/s]\n",
      "\u001b[Acessed prompts:   2% 16/1024 [00:12<04:26,  3.79it/s, est. speed input: 3797.69 toks/s, output: 121.52 toks/s]\n",
      "\u001b[Acessed prompts:   2% 17/1024 [00:12<03:57,  4.24it/s, est. speed input: 3988.57 toks/s, output: 132.96 toks/s]\n",
      "\u001b[Acessed prompts:   2% 19/1024 [00:12<02:58,  5.63it/s, est. speed input: 4394.52 toks/s, output: 156.88 toks/s]\n",
      "\u001b[Acessed prompts:   2% 21/1024 [00:12<02:19,  7.21it/s, est. speed input: 4803.48 toks/s, output: 181.06 toks/s]\n",
      "\u001b[Acessed prompts:   2% 23/1024 [00:12<02:01,  8.23it/s, est. speed input: 5188.09 toks/s, output: 204.74 toks/s]\n",
      "\u001b[Acessed prompts:   2% 25/1024 [00:13<02:59,  5.57it/s, est. speed input: 5385.84 toks/s, output: 223.85 toks/s]\n",
      "\u001b[Acessed prompts:   3% 26/1024 [00:13<03:41,  4.51it/s, est. speed input: 5436.77 toks/s, output: 232.00 toks/s]\n",
      "\u001b[Acessed prompts:   3% 27/1024 [00:13<03:24,  4.88it/s, est. speed input: 5588.09 toks/s, output: 244.48 toks/s]\n",
      "\u001b[Acessed prompts:   3% 29/1024 [00:14<03:17,  5.03it/s, est. speed input: 5842.15 toks/s, output: 267.77 toks/s]\n",
      "\u001b[Acessed prompts:   3% 30/1024 [00:14<03:56,  4.21it/s, est. speed input: 5886.07 toks/s, output: 276.60 toks/s]\n",
      "\u001b[Acessed prompts:   3% 31/1024 [00:15<05:11,  3.19it/s, est. speed input: 5855.59 toks/s, output: 282.83 toks/s]\n",
      "\u001b[Acessed prompts:   3% 32/1024 [00:15<04:55,  3.36it/s, est. speed input: 5946.90 toks/s, output: 295.04 toks/s]\n",
      "\u001b[Acessed prompts:   3% 33/1024 [00:15<04:07,  4.00it/s, est. speed input: 6087.20 toks/s, output: 309.70 toks/s]\n",
      "\u001b[Acessed prompts:   3% 34/1024 [00:16<05:18,  3.11it/s, est. speed input: 6071.70 toks/s, output: 317.25 toks/s]\n",
      "\u001b[Acessed prompts:   3% 35/1024 [00:16<04:35,  3.59it/s, est. speed input: 6184.90 toks/s, output: 331.49 toks/s]\n",
      "\u001b[Acessed prompts:   4% 37/1024 [00:16<03:17,  5.00it/s, est. speed input: 6457.22 toks/s, output: 362.44 toks/s]\n",
      "\u001b[Acessed prompts:   4% 38/1024 [00:16<03:16,  5.02it/s, est. speed input: 6553.44 toks/s, output: 375.96 toks/s]\n",
      "\u001b[Acessed prompts:   4% 39/1024 [00:16<02:55,  5.62it/s, est. speed input: 6678.85 toks/s, output: 391.20 toks/s]\n",
      "\u001b[Acessed prompts:   4% 40/1024 [00:17<03:20,  4.90it/s, est. speed input: 6739.53 toks/s, output: 402.97 toks/s]\n",
      "\u001b[Acessed prompts:   4% 41/1024 [00:17<04:29,  3.64it/s, est. speed input: 6726.52 toks/s, output: 395.93 toks/s]\n",
      "\u001b[Acessed prompts:   4% 42/1024 [00:17<03:45,  4.35it/s, est. speed input: 6844.65 toks/s, output: 402.74 toks/s]\n",
      "\u001b[Acessed prompts:   4% 43/1024 [00:18<04:58,  3.28it/s, est. speed input: 6817.84 toks/s, output: 410.85 toks/s]\n",
      "\u001b[Acessed prompts:   4% 45/1024 [00:18<04:13,  3.85it/s, est. speed input: 6976.99 toks/s, output: 436.53 toks/s]\n",
      "\u001b[Acessed prompts:   4% 46/1024 [00:18<05:09,  3.16it/s, est. speed input: 6947.31 toks/s, output: 444.93 toks/s]\n",
      "\u001b[Acessed prompts:   5% 48/1024 [00:19<04:39,  3.49it/s, est. speed input: 7067.84 toks/s, output: 458.13 toks/s]\n",
      "\u001b[Acessed prompts:   5% 49/1024 [00:19<04:08,  3.93it/s, est. speed input: 7161.39 toks/s, output: 468.99 toks/s]\n",
      "\u001b[Acessed prompts:   5% 51/1024 [00:19<03:04,  5.28it/s, est. speed input: 7386.70 toks/s, output: 504.83 toks/s]\n",
      "\u001b[Acessed prompts:   5% 53/1024 [00:20<02:48,  5.77it/s, est. speed input: 7565.80 toks/s, output: 537.68 toks/s]\n",
      "\u001b[Acessed prompts:   5% 54/1024 [00:20<03:24,  4.74it/s, est. speed input: 7574.06 toks/s, output: 532.87 toks/s]\n",
      "\u001b[Acessed prompts:   5% 55/1024 [00:20<03:09,  5.11it/s, est. speed input: 7660.53 toks/s, output: 544.71 toks/s]\n",
      "\u001b[Acessed prompts:   5% 56/1024 [00:20<03:09,  5.10it/s, est. speed input: 7725.83 toks/s, output: 560.14 toks/s]\n",
      "\u001b[Acessed prompts:   6% 57/1024 [00:21<04:03,  3.97it/s, est. speed input: 7711.23 toks/s, output: 570.11 toks/s]\n",
      "\u001b[Acessed prompts:   6% 58/1024 [00:21<04:17,  3.76it/s, est. speed input: 7735.03 toks/s, output: 574.29 toks/s]\n",
      "\u001b[Acessed prompts:   6% 60/1024 [00:21<02:50,  5.64it/s, est. speed input: 7955.77 toks/s, output: 613.03 toks/s]\n",
      "\u001b[Acessed prompts:   6% 61/1024 [00:21<02:43,  5.91it/s, est. speed input: 8034.94 toks/s, output: 625.34 toks/s]\n",
      "\u001b[Acessed prompts:   6% 63/1024 [00:22<03:25,  4.68it/s, est. speed input: 8090.85 toks/s, output: 633.94 toks/s]\n",
      "\u001b[Acessed prompts:   6% 65/1024 [00:22<02:31,  6.32it/s, est. speed input: 8301.45 toks/s, output: 672.96 toks/s]\n",
      "\u001b[Acessed prompts:   6% 66/1024 [00:22<02:44,  5.81it/s, est. speed input: 8345.45 toks/s, output: 672.49 toks/s]\n",
      "\u001b[Acessed prompts:   7% 67/1024 [00:24<09:06,  1.75it/s, est. speed input: 7811.60 toks/s, output: 641.74 toks/s]\n",
      "\u001b[Acessed prompts:   7% 68/1024 [00:25<08:37,  1.85it/s, est. speed input: 7785.49 toks/s, output: 652.07 toks/s]\n",
      "\u001b[Acessed prompts:   7% 70/1024 [00:25<06:40,  2.38it/s, est. speed input: 7859.04 toks/s, output: 682.97 toks/s]"
     ]
    }
   ],
   "source": [
    "syn_ds = ds.map(generate_content_base, batched=True, batch_size=BATCH_SIZE, input_columns=[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb5065-f94d-42a7-a143-cc547fd24895",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_ds.push_to_hub('amang1802/cpt_gen_content_topic_conditioned_L3.1_8B_qna')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc741286-3bec-4716-b39c-51efe2dd9dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
