{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2add563c-7125-43ed-a566-90a1fc2ba873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from jinja2 import Template\n",
    "from transformers import AutoTokenizer\n",
    "from pydantic import BaseModel, TypeAdapter\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.sampling_params import GuidedDecodingParams\n",
    "\n",
    "import json\n",
    "import random\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e016f04-5a40-4355-825c-a8f7870069f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GPUS = 4\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7351937-b445-4661-a22b-235ddb092544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#persona_content_ds = load_dataset('amang1802/wiki_topic_persona_sampled_405B')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85cc5e66-8c42-47f4-b3cc-07c4b41dfcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text_to_persona_ds(ds):\n",
    "    id_to_text = {}\n",
    "    for i in range(content_ds.num_rows):\n",
    "        id_to_text[content_ds[i]['id']] = content_ds[i]['text']\n",
    "\n",
    "    return ds.map(lambda idx: {\"text\": id_to_text[idx]}, input_columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec3a42f6-ee2f-4587-bcce-2e230e799fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#persona_content_ds = add_text_to_persona_ds(persona_content_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9253e2a5-4858-42bd-a287-55b2174fe348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_one_per_persona_ds(ds):\n",
    "    uniq_personas = list(set(ds['persona_id']))\n",
    "    uniq_contents = list(set(ds['id']))\n",
    "    included_pairs = [(cid, random.choice(uniq_personas)) for cid in uniq_contents]\n",
    "\n",
    "    return ds.filter(lambda row: (row['id'], row['persona_id']) in included_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa14252e-5274-4a7a-b946-8dffa14ecf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#persona_uniq_ds = pick_one_per_persona_ds(persona_content_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8c35780-3b31-48ed-af7d-846f96202cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#persona_uniq_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d5c9c51-bfde-4de7-80cb-c27776c57fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert persona_uniq_ds.num_rows == len(set(persona_uniq_ds['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ccd6055-e602-490e-805c-1b213c1848eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"gt_accuracy.jinja2\") as f:\n",
    "    template_str = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b223e6ae-5555-4ffa-8eb2-a216e84a72a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"few_shots.json\") as f:\n",
    "    examples_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90c1ba71-3723-4d34-8f31-f4ecf895f419",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in examples_json:\n",
    "    example['matches'] = json.dumps(example['matches'], indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "791fe680-6e1b-419d-a4e1-f1d8c7dcbd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = Template(template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a8c3a1b-b200-49f0-85d2-888ed994988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = template.render(examples=examples_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd7b013e-21b7-4b90-bba5-0948d75ebc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Instructions\n",
      "\n",
      "You are a fact checker and you're required to compare a pair of texts and find segments that discuss the same facts and judge if they both match on the facts. The goal is to only judge the alignment on facts stated by both. They can state unique facts which we have to ignore. For a pair of texts, output the a list of common segments and if they match or not.\n",
      "\n",
      "On the inclusion of segments:\n",
      "- Inspect every sentence in text1 and text2 and include all segments that discuss common facts.\n",
      "- If one of the text has a segment with no similar segment in the other text, ignore that segment altogether.\n",
      "- Do not pair segments that have different facts. For example: If one says Roger Federer won the Wimbledon in 2003, and the other says Roger Federer won the French Open in 2009 - they are different facts and shouldn't be paired together.\n",
      "- Repeating this instruction: Include all segments that discuss common facts.\n",
      "\n",
      "On the matching sensitivity:\n",
      "- It's possible that two statements don't exactly agree but are close enough. Like if one says the length as 50cm, another 55cm, or age in 50s and the other in the early 60s. Mark that match as true.\n",
      "- It's possible that one statement has extra information. Like one might say he is a scientist, but the other says that he is a scientist and an inventor. Extra information that's congruent with each other is a true match.\n",
      "- Use a smartly assessed judgement to mark the match as true. Do not point out the smallest differences and mark the match as false.\n",
      "\n",
      "# Output format\n",
      "\n",
      "Output a JSON in the following format:\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"text1\": \"<segment from text1>\",\n",
      "    \"text2\": \"<segment from text2>\",\n",
      "    \"rationale\": \"<explanation why segments match or don't match>\n",
      "    \"match\": <true|false>\n",
      "  },\n",
      "  ...\n",
      "]\n",
      "\n",
      "Do not any include any preamble or notes. This will be used by an automated parsing system - so output only the JSON.\n",
      "\n",
      "# Examples\n",
      "\n",
      "text1:\n",
      "Average life span in the wild: 12 years Size: 21 in (50 cm) Weight: 14.4 oz (408 g) Did you know? Chameleons don't change colors to match their surroundings. Each species displays distinct color patterns to indicate specific reactions or emotions. The Meller's chameleon is the largest of the chameleons not native to Madagascar. Their stout bodies can grow to be up to two feet (two-thirds of a meter) long and weigh more than a pound (one-half kilogram). Meller's distinguish themselves from their universally bizarre-looking cousins with a single small horn protruding from the front of their snouts. This and their size earn them the common name 'giant one-horned chameleon.' They are fairly common in the savanna of East Africa, including Malawi, northern Mozambique, and Tanzania. Almost one-half of the world’s chameleons live on the island of Madagascar. As with all chameleons, Meller's will change colors in response to stress and to communicate with other chameleons. Their normal appearance is deep green with yellow stripes and random black spots. Females are slightly smaller, but are otherwise indistinguishable from males. They subsist on insects and small birds, using their camouflage and a lightning-fast, catapulting tongue, which can be up to 20 inches (50 centimeters) long, to ambush prey. Exotic pet enthusiasts often attempt to keep Meller's chameleons as pets. However, they are highly susceptible to even the slightest level of stress and are very difficult to care for in captivity. In the wild, they can live as long as 12 years.\n",
      "\n",
      "text2:\n",
      "The Meller's chameleon exemplifies the fascinating diversity within the family Chamaeleonidae. With an average lifespan of 12 years in the wild, these remarkable reptiles are distinguished by two horns, one large and one small, on their snout and display a deep green coloration with yellow stripes and black spots. They inhabit the savannas of East Africa, specifically in Malawi and Tanzania.\n",
      "The taxonomy of chameleons has undergone significant changes since 1986, when the family Chamaeleonidae was initially divided into two subfamilies: Brookesiinae and Chamaeleoninae. Under this classification, Brookesiinae included the genera Brookesia and Rhampholeon, along with Palleon and Rieppeleon, while Chamaeleoninae encompassed Bradypodion, Calumma, Chamaeleo, Furcifer, and Trioceros, as well as Archaius, Nadzikambia, and Kinyongia.\n",
      "Like their relatives, Meller's chameleons are skilled hunters, feeding on insects and small birds using their remarkable tongue that can extend up to 50 inches. Chameleons change colors to match their surroundings; but, their color also serves as a means of communication and emotional expression.. This behavioral trait exists across the family, regardless of taxonomic classification.\n",
      "The subfamilial division has been subject to considerable debate, with most phylogenetic studies indicating that the pygmy chameleons of Brookesiinae are not a monophyletic group. While some authorities initially maintained this classification based on the absence of evidence principle, they later abandoned the subfamilial division entirely. However, in 2015, Glaw proposed a revised classification, placing only Brookesia and Palleon within Brookesiinae, while all other genera were assigned to Chamaeleoninae.\n",
      "While these chameleons are fascinating subjects for scientific study, they prove challenging as pets, being highly sensitive to stress and requiring specialized care in captivity. This sensitivity reflects the complex nature of these remarkable reptiles, whose classification continues to evolve as our understanding of their relationships deepens.\n",
      "\n",
      "response:\n",
      "[\n",
      "  {\n",
      "    \"text1\": \"Average life span in the wild: 12 years\",\n",
      "    \"text2\": \"With an average lifespan of 12 years in the wild\",\n",
      "    \"rationale\": \"Both are discussing the lifespan and agree that it is 12 years in the wild.\",\n",
      "    \"match\": true\n",
      "  },\n",
      "  {\n",
      "    \"text1\": \"Meller's distinguish themselves from their universally bizarre-looking cousins with a single small horn protruding from the front of their snouts\",\n",
      "    \"text2\": \"these remarkable reptiles are distinguished by two horns, one large and one small, on their snout\",\n",
      "    \"rationale\": \"They both discuss the horn on the snout but disagree on the count and the size of the horns.\",\n",
      "    \"match\": false\n",
      "  },\n",
      "  {\n",
      "    \"text1\": \"They are fairly common in the savanna of East Africa, including Malawi, northern Mozambique, and Tanzania.\",\n",
      "    \"text2\": \"They inhabit the savannas of East Africa, specifically in Malawi and Tanzania.\",\n",
      "    \"rationale\": \"They are discussing the habitat and agree on the regions mentioned in both - Malawi and Tanzania.\",\n",
      "    \"match\": true\n",
      "  },\n",
      "  {\n",
      "    \"text1\": \"They subsist on insects and small birds, using their camouflage and a lightning-fast, catapulting tongue, which can be up to 20 inches (50 centimeters) long, to ambush prey\",\n",
      "    \"text2\": \"Meller's chameleons are skilled hunters, feeding on insects and small birds using their remarkable tongue that can extend up to 50 inches\",\n",
      "    \"rationale\": \"Both discuss the hunting habits bit disagree on the specific length of their tongues.\",\n",
      "    \"match\": false\n",
      "  },\n",
      "  {\n",
      "    \"text1\": \"Chameleons don't change colors to match their surroundings. Each species displays distinct color patterns to indicate specific reactions or emotions.\",\n",
      "    \"text2\": \"Chameleons change colors to match their surroundings; but, their color also serves as a means of communication and emotional expression.\",\n",
      "    \"rationale\": \"They are addressing the topic of Chameleons color changes and disagree that it's due to camouflage\",\n",
      "    \"match\": false\n",
      "  },\n",
      "  {\n",
      "    \"text1\": \"Exotic pet enthusiasts often attempt to keep Meller's chameleons as pets. However, they are highly susceptible to even the slightest level of stress and are very difficult to care for in captivity\",\n",
      "    \"text2\": \"they prove challenging as pets, being highly sensitive to stress and requiring specialized care in captivity\",\n",
      "    \"rationale\": \"Both discuss how it is to keep chameleons as pets and state that it's challenging due to their sensitivity to stress.\",\n",
      "    \"match\": true\n",
      "  }\n",
      "]\n",
      "---\n",
      "\n",
      "text1:\n",
      "The Chinook Arch November 14, 2001 The above photo was taken at evening twilight in Calgary, Alberta by Jeff McIntosh. On the lee (eastern) side of the Rocky Mountains in Colorado, Wyoming, Montana, and the province of Alberta in Canada, chinook winds occasionally bring respite from cold weather. Chinook is an Indian name meaning 'snow eater.' These warm, westerly winds result from downslope winds - air moving across the Rocky Mountains and down onto the prairies. During those cold, dull gray winter days, Albertans sometimes look toward the mountains for the Chinook Arch, a curved patch of blue sky (as shown above) that indicates that warm winds are approaching. Over this past weekend, a strong chinook was felt in Alberta and Montana. Chinooks typically occur from early November to late March.\n",
      "\n",
      "text2:\n",
      "The interior Chinook is a föhn wind that develops in a rain shadow. When air rises up windward slopes (orographic lift), it loses most of its moisture, and its subsequent adiabatic warming creates this wind. The adiabatic rates differ between moist and dry air, causing air on leeward slopes to become warmer than at matching elevations on windward slopes.\n",
      "The interior Chinooks and coastal Chinooks can sometimes share the same air flow source. When Pacific winds (coastal Chinooks) encounter mountains, they're forced upward. This causes the air's moisture to condense and fall as precipitation, while cooling at the moist adiabatic rate of 5 °C / 1000 m (3.5 °F / 1000 ft). As this now-dried air descends the mountains' leeward side, it warms at the dry adiabatic rate of 10 °C / 1000 m (5.5 °F / 1000 ft).\n",
      "The strong winds' turbulence can prevent the typical nighttime temperature inversion from forming on the lee side of the slope, keeping temperatures higher at night.A single air flow can create three distinct weather patterns: rain drenching the Pacific Northwest coast, snow hammering the Rockies' windward (western) side (removing moisture from the air), and a föhn Chinook warming the Rockies' leeward (eastern) side in Alberta. This shared air flow source explains why there's confusion about the term 'Chinook wind'.\n",
      "\n",
      "response:\n",
      "[\n",
      "  {\n",
      "    \"text1\": \"On the lee (eastern) side of the Rocky Mountains in Colorado, Wyoming, Montana, and the province of Alberta in Canada, chinook winds occasionally bring respite from cold weather\",\n",
      "    \"text2\": \"The adiabatic rates differ between moist and dry air, causing air on leeward slopes to become warmer than at matching elevations on windward slopes...As this now-dried air descends the mountains' leeward side, it warms at the dry adiabatic rate of 10 \\u00b0C / 1000 m (5.5 \\u00b0F / 1000 ft)\",\n",
      "    \"rationale\": \"They discuss the impact of chinook winds on leeward side and that the air is warmer on that side of the mountain.\",\n",
      "    \"match\": true\n",
      "  }\n",
      "]\n",
      "---\n",
      "\n",
      "text1:\n",
      "As I become more and more interested in American history and archaeology, I found this latest news about the USS Monitor quite fascinating: The Monitor finally sank around 1 a.m. on December 31. Twelve sailors and four officers would lose their lives. Periodicals like Harper's Weekly and Frank Leslie's Illustrated Newspaper would later publish artists' renderings and poems about the tragedy, but for families of the victims there was little solace. The exact location of the Monitor's final resting place and the crewmen who perished would remain a mystery for more than a century. John Byrd, director of the laboratory, says that 'sunken ships can be a very, very good environment for preserving remains' because of the protective coating of silt that forms over them. This was the case inside the Monitor, where tons of coal mixed with the silt, creating an anaerobic environment that prevented chemical reactions and animal activity from destroying the skeletons.\n",
      "\n",
      "text2:\n",
      "On December 24, 1862, Monitor was ordered to join a blockade off Charleston via Beaufort, North Carolina. The crew, some of whom had experienced her difficult March voyage, received the orders on Christmas Day. Under Commander John P. Bankhead, Monitor departed on December 31, towed by USS Rhode Island. As a storm intensified off Cape Hatteras, Bankhead used a blackboard to communicate that Monitor would signal with a red lantern if help was needed.\n",
      "Monitor capsized and sank stern-first around 1:30 AM on January 1, 1863, sixteen miles southeast of Cape Hatteras, losing twenty men, including four officers trapped in the detaching turret. Forty-seven men were rescued by Rhode Island's lifeboats, including Bankhead, Greene, and Stodder, who suffered from exposure. The Navy accepted the officers' reports without inquiry and took no disciplinary action.\n",
      "In August 1973, a Duke University expedition led by John G. Newton discovered Monitor's wreck at 230 feet depth (35°0′6″N 75°24′23″W). Initial photography attempts failed, but they eventually confirmed the ship was lying upside down. The discovery was announced on March 8, 1974, and confirmed that month by the submersible Alcoa Sea Probe.\n",
      "In 2002, about 160 divers worked to recover the 120-ton turret, which was filled with concreted coal from a ruptured bunker. Using various tools to clear debris, they installed a reinforced platform and an eight-legged lifting frame called the 'spider.' During the operation, divers found one skeleton in the turret, managing to recover half of it from the concreted debris while the other half remained inaccessible behind a gun.\n",
      "\n",
      "response:\n",
      "[\n",
      "  {\n",
      "    \"text1\": \"The Monitor finally sank around 1 a.m. on December 31.\",\n",
      "    \"text2\": \"Under Commander John P. Bankhead, Monitor departed on December 31, towed by USS Rhode Island. Monitor capsized and sank stern-first around 1:30 AM on January 1, 1863, sixteen miles southeast of Cape Hatteras\",\n",
      "    \"rationale\": \"The discuss the timing of the shinking of the ship USS Monitor and agree on the date December 31 and time at about 1 AM.\",\n",
      "    \"match\": true\n",
      "  },\n",
      "  {\n",
      "    \"text1\": \"Twelve sailors and four officers would lose their lives.\",\n",
      "    \"text2\": \"losing twenty men, including four officers trapped in the detaching turret\",\n",
      "    \"rationale\": \"They mention that lives were lost but disagree on how many lives were lost.\",\n",
      "    \"match\": false\n",
      "  },\n",
      "  {\n",
      "    \"text1\": \"The exact location of the Monitor's final resting place and the crewmen who perished would remain a mystery for more than a century\",\n",
      "    \"text2\": \"On December 24, 1862, Monitor was ordered to join a blockade....In August 1973, a Duke University expedition led by John G. Newton discovered Monitor's wreck\",\n",
      "    \"rationale\": \"Both discuss the time period between the sinking and discovery of the wreck, stating that it took more than 100 years to discover the wreck after its sinking.\",\n",
      "    \"match\": true\n",
      "  }\n",
      "]\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83d4947d-52f8-43ae-a920-5d4e0373690c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen2.5-72B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ee9aaf8-5f8e-43a1-80a3-997a32771a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Judgement(BaseModel):\n",
    "    text1: str\n",
    "    text2: str\n",
    "    rationale: str\n",
    "    match: bool\n",
    "\n",
    "ta = TypeAdapter(list[Judgement])\n",
    "\n",
    "json_schema = ta.json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "177c74e1-107b-4356-9485-8f5a928492e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-28 05:34:33 config.py:478] This model supports multiple tasks: {'generate', 'embed', 'classify', 'score', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 12-28 05:34:33 config.py:1216] Defaulting to use mp for distributed inference\n",
      "INFO 12-28 05:34:33 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='Qwen/Qwen2.5-72B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-72B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=24576, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-72B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 12-28 05:34:33 multiproc_worker_utils.py:312] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 12-28 05:34:33 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 12-28 05:34:34 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167958)\u001b[0;0m INFO 12-28 05:34:34 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167958)\u001b[0;0m INFO 12-28 05:34:34 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167957)\u001b[0;0m INFO 12-28 05:34:34 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167957)\u001b[0;0m INFO 12-28 05:34:34 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167959)\u001b[0;0m INFO 12-28 05:34:34 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167959)\u001b[0;0m INFO 12-28 05:34:34 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "INFO 12-28 05:34:37 utils.py:922] Found nccl from library libnccl.so.2\n",
      "INFO 12-28 05:34:37 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167957)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=167958)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=167959)\u001b[0;0m INFO 12-28 05:34:37 utils.py:922] Found nccl from library libnccl.so.2\n",
      "INFO 12-28 05:34:37 utils.py:922] Found nccl from library libnccl.so.2\n",
      "INFO 12-28 05:34:37 utils.py:922] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167957)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=167958)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=167959)\u001b[0;0m INFO 12-28 05:34:37 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-28 05:34:37 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-28 05:34:37 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-28 05:34:39 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167957)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=167958)\u001b[0;0m INFO 12-28 05:34:39 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 12-28 05:34:39 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167959)\u001b[0;0m INFO 12-28 05:34:39 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 12-28 05:34:39 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_9cdb0f04'), local_subscribe_port=35377, remote_subscribe_port=None)\n",
      "INFO 12-28 05:34:39 model_runner.py:1092] Starting to load model Qwen/Qwen2.5-72B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167957)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=167958)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=167959)\u001b[0;0m INFO 12-28 05:34:39 model_runner.py:1092] Starting to load model Qwen/Qwen2.5-72B-Instruct...\n",
      "INFO 12-28 05:34:39 model_runner.py:1092] Starting to load model Qwen/Qwen2.5-72B-Instruct...\n",
      "INFO 12-28 05:34:39 model_runner.py:1092] Starting to load model Qwen/Qwen2.5-72B-Instruct...\n",
      "INFO 12-28 05:34:39 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167958)\u001b[0;0m INFO 12-28 05:34:39 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167957)\u001b[0;0m INFO 12-28 05:34:39 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167959)\u001b[0;0m INFO 12-28 05:34:39 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a88c17e8c994f4d9ff24f4b4f914f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/37 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=167958)\u001b[0;0m INFO 12-28 05:35:09 model_runner.py:1097] Loading model weights took 33.9833 GB\n",
      "INFO 12-28 05:35:09 model_runner.py:1097] Loading model weights took 33.9833 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167957)\u001b[0;0m INFO 12-28 05:35:09 model_runner.py:1097] Loading model weights took 33.9833 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167959)\u001b[0;0m INFO 12-28 05:35:09 model_runner.py:1097] Loading model weights took 33.9833 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167958)\u001b[0;0m INFO 12-28 05:35:13 worker.py:241] Memory profiling takes 3.74 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167958)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=167957)\u001b[0;0m INFO 12-28 05:35:13 worker.py:241] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.98) = 136.92GiB\n",
      "INFO 12-28 05:35:13 worker.py:241] Memory profiling takes 3.74 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167958)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=167957)\u001b[0;0m INFO 12-28 05:35:13 worker.py:241] model weights take 33.98GiB; non_torch_memory takes 4.14GiB; PyTorch activation peak memory takes 2.92GiB; the rest of the memory reserved for KV Cache is 95.88GiB.\n",
      "INFO 12-28 05:35:13 worker.py:241] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.98) = 136.92GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167957)\u001b[0;0m INFO 12-28 05:35:13 worker.py:241] model weights take 33.98GiB; non_torch_memory takes 4.14GiB; PyTorch activation peak memory takes 2.92GiB; the rest of the memory reserved for KV Cache is 95.88GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167959)\u001b[0;0m INFO 12-28 05:35:13 worker.py:241] Memory profiling takes 3.77 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167959)\u001b[0;0m INFO 12-28 05:35:13 worker.py:241] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.98) = 136.92GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167959)\u001b[0;0m INFO 12-28 05:35:13 worker.py:241] model weights take 33.98GiB; non_torch_memory takes 3.67GiB; PyTorch activation peak memory takes 2.92GiB; the rest of the memory reserved for KV Cache is 96.35GiB.\n",
      "INFO 12-28 05:35:13 worker.py:241] Memory profiling takes 3.80 seconds\n",
      "INFO 12-28 05:35:13 worker.py:241] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.98) = 136.92GiB\n",
      "INFO 12-28 05:35:13 worker.py:241] model weights take 33.98GiB; non_torch_memory takes 4.92GiB; PyTorch activation peak memory takes 2.92GiB; the rest of the memory reserved for KV Cache is 95.09GiB.\n",
      "INFO 12-28 05:35:13 distributed_gpu_executor.py:57] # GPU blocks: 77900, # CPU blocks: 3276\n",
      "INFO 12-28 05:35:13 distributed_gpu_executor.py:61] Maximum concurrency for 24576 tokens per request: 50.72x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167958)\u001b[0;0m INFO 12-28 05:35:16 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167958)\u001b[0;0m INFO 12-28 05:35:16 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167957)\u001b[0;0m INFO 12-28 05:35:16 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167957)\u001b[0;0m INFO 12-28 05:35:16 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-28 05:35:16 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-28 05:35:16 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167959)\u001b[0;0m INFO 12-28 05:35:16 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167959)\u001b[0;0m INFO 12-28 05:35:16 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167959)\u001b[0;0m INFO 12-28 05:35:30 custom_all_reduce.py:224] Registering 5635 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167958)\u001b[0;0m INFO 12-28 05:35:31 custom_all_reduce.py:224] Registering 5635 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167957)\u001b[0;0m INFO 12-28 05:35:31 custom_all_reduce.py:224] Registering 5635 cuda graph addresses\n",
      "INFO 12-28 05:35:31 custom_all_reduce.py:224] Registering 5635 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167959)\u001b[0;0m INFO 12-28 05:35:31 model_runner.py:1527] Graph capturing finished in 15 secs, took 0.45 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167957)\u001b[0;0m INFO 12-28 05:35:31 model_runner.py:1527] Graph capturing finished in 15 secs, took 0.45 GiB\n",
      "INFO 12-28 05:35:31 model_runner.py:1527] Graph capturing finished in 15 secs, took 0.45 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=167958)\u001b[0;0m INFO 12-28 05:35:31 model_runner.py:1527] Graph capturing finished in 16 secs, took 0.45 GiB\n",
      "INFO 12-28 05:35:31 llm_engine.py:446] init engine (profile, create kv cache, warmup model) took 22.12 seconds\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=model_id, max_model_len=24576, tensor_parallel_size=NUM_GPUS, gpu_memory_utilization=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "478c8c2c-485a-44ba-8bda-8c7e128b3175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gt_accuracy(gt_texts, synthetic_texts):\n",
    "    messages = [[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": \"text1:\\n\" + text1 + \"\\n\\ntext2:\\n\" + text2 + \"\\n\\nresponse:\" }]\n",
    "                for text1, text2 in zip(gt_texts, synthetic_texts)]\n",
    "\n",
    "    guided_decoding_params = GuidedDecodingParams(json=json_schema)\n",
    "    outputs = llm.chat(messages, SamplingParams(temperature=0.3, top_p=0.9, max_tokens=1536, guided_decoding=guided_decoding_params))\n",
    "\n",
    "    judgements = []\n",
    "    scores = []\n",
    "    for output in outputs:\n",
    "        response = output.outputs[0].text.strip()\n",
    "        judgement = []\n",
    "        score = -1.0\n",
    "        try:\n",
    "            judgement = json.loads(response)\n",
    "            num_matches = sum([1 for j in judgement if j['match']])\n",
    "            score = num_matches / len(judgement) if len(judgement) > 0 else -1\n",
    "        except Exception:\n",
    "            pass\n",
    "            #print(traceback.format_exc())\n",
    "\n",
    "        judgements.append(judgement)\n",
    "        scores.append(score)\n",
    "        \n",
    "    return {\n",
    "        \"judgement\": judgements,\n",
    "        \"accuracy_score\": scores\n",
    "    }   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9987af43-1b4d-476d-8a8d-af739613a805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(ds):\n",
    "    valid_scores = [score for score in ds['accuracy_score'] if score >= 0]\n",
    "    return sum(valid_scores) / len(valid_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d364e615-f6a6-4bec-8aa2-1b86a61beae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#judged_ds1 = persona_uniq_ds.map(compute_gt_accuracy, input_columns=['text', 'synthetic_content'], batched=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a83293e7-501e-4c7d-9bab-7e05ecf815ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_score(judged_ds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79750e55-2108-4ef9-b886-ebcda8624dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#judged_ds1.push_to_hub('amang1802/wiki_topic_persona_405B_uniq_gt_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0ee8f13-1a8c-4726-a7b3-0c0d2f3ab847",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_list = [\n",
    "    'amang1802/synthetic_data_topic_conditioned_L3.3_70B',\n",
    "    'amang1802/synthetic_data_prefix_conditioned_L3.3_70B',\n",
    "    'amang1802/synthetic_data_fulltext_conditioned_L3.3_70B'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e927fc-a607-4b15-818a-9510cfe59aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e14b01185884b2abf5a538620014c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Acessed prompts:   0% 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "\u001b[Acessed prompts:   0% 1/512 [01:41<14:25:08, 101.58s/it, est. speed input: 40.99 toks/s, output: 1.00 toks/s]\n",
      "\u001b[Acessed prompts:   0% 2/512 [01:42<6:02:29, 42.65s/it, est. speed input: 83.15 toks/s, output: 2.07 toks/s]  \n",
      "\u001b[Acessed prompts:   1% 3/512 [01:43<3:19:03, 23.46s/it, est. speed input: 122.73 toks/s, output: 3.15 toks/s]\n",
      "\u001b[Acessed prompts:   1% 4/512 [01:45<2:08:09, 15.14s/it, est. speed input: 159.59 toks/s, output: 4.28 toks/s]\n",
      "\u001b[Acessed prompts:   1% 6/512 [01:47<1:03:40,  7.55s/it, est. speed input: 241.06 toks/s, output: 6.67 toks/s]\n",
      "\u001b[Acessed prompts:   2% 8/512 [01:48<38:29,  4.58s/it, est. speed input: 329.75 toks/s, output: 9.14 toks/s]  \n",
      "\u001b[Acessed prompts:   2% 9/512 [01:49<30:31,  3.64s/it, est. speed input: 373.86 toks/s, output: 10.41 toks/s]"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "for ds in ds_list:\n",
    "    content_ds = load_dataset(ds)['train']\n",
    "    judged_ds = content_ds.map(compute_gt_accuracy, input_columns=['text', 'synthetic_content'], batched=True, batch_size=BATCH_SIZE)\n",
    "    judged_ds.push_to_hub(ds)\n",
    "    scores[ds] = get_score(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32d7582-fcac-401a-948c-87ac56459700",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed913b21-a140-4805-b8d1-3e3a8e569361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
